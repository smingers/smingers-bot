# Metaculus AI Forecasting Bot Configuration
# 5-Agent Ensemble Pipeline

# =============================================================================
# RUN MODE
# =============================================================================
# Controls model selection and submission behavior:
#   - "test": Cheap models (Haiku), no submission - for testing pipeline
#   - "preview": Production models, no submission - for evaluating quality
#   - "live": Production models, submits to Metaculus
#
# Override with --mode flag: python main.py --question 12345 --mode live

mode: "test"

# =============================================================================
# MODEL CONFIGURATION
# =============================================================================
# Models used for research query generation and other utility tasks.
# All models accessed via OpenRouter (set OPENROUTER_API_KEY).
#
# The 5-agent ensemble is configured separately below.

models:
  # Models for test mode (fast, inexpensive)
  fast:
    query_generator: "openrouter/openai/gpt-4o-mini"
    article_summarizer: "openrouter/openai/gpt-4o-mini"
    agentic_search: "openrouter/openai/gpt-4o-mini"

  # Models for preview and live modes (higher quality)
  quality:
    query_generator: "openrouter/openai/o3"
    article_summarizer: "openrouter/anthropic/claude-sonnet-4.6"
    agentic_search: "openrouter/openai/o3"

# =============================================================================
# 5-AGENT ENSEMBLE CONFIGURATION
# =============================================================================
# The ensemble uses 5 forecasting agents with configurable weights and model 
# selection
#
# Cross-pollination structure (creates cross-model diversity):
#   - Agent 1 (Sonnet 4.6) receives Agent 1's step 1 output (Sonnet 4.6)
#   - Agent 2 (Sonnet 4.6) receives Agent 5's step 1 output (o3)
#   - Agent 3 (GPT-5.2) receives Agent 1's step 1 output (Sonnet 4.6)
#   - Agent 4 (o3) receives Agent 3's step 1 output (GPT-5.2)
#   - Agent 5 (o3) receives Agent 5's step 1 output (o3)

ensemble:
  # Agents for test mode (all gpt-4o-mini - fast, inexpensive)
  fast:
    - name: "forecaster_1"
      model: "openrouter/openai/gpt-4o-mini"
      weight: 1.0

    - name: "forecaster_2"
      model: "openrouter/openai/gpt-4o-mini"
      weight: 1.0

    - name: "forecaster_3"
      model: "openrouter/openai/gpt-4o-mini"
      weight: 1.0

    - name: "forecaster_4"
      model: "openrouter/openai/gpt-4o-mini"
      weight: 1.0

    - name: "forecaster_5"
      model: "openrouter/openai/gpt-4o-mini"
      weight: 1.0

  # Agents for preview and live modes (higher quality)
  quality:
    - name: "forecaster_1"
      model: "openrouter/anthropic/claude-sonnet-4.6"
      weight: 1.0

    - name: "forecaster_2"
      model: "openrouter/anthropic/claude-sonnet-4.6"
      weight: 1.0

    - name: "forecaster_3"
      model: "openrouter/openai/gpt-5.2"
      weight: 1.0

    - name: "forecaster_4"
      model: "openrouter/openai/o3"
      weight: 1.0

    - name: "forecaster_5"
      model: "openrouter/openai/o3"
      weight: 1.0

  aggregation: "weighted_average"

# =============================================================================
# RESEARCH CONFIGURATION
# =============================================================================
# Research is integrated into each handler. The pipeline:
# 1. Generate historical queries (outside view context)
# 2. Generate current queries (inside view context)
# 3. Execute searches via Google (Serper) and AskNews
# 4. Extract and summarize content

research:
  # Iterative research planner (replaces legacy 2-step query generation)
  # When enabled, uses a unified plan-execute-reflect loop instead of
  # independent historical + current query generation
  iterative_planner_enabled: true

  # Planner configuration (only used when iterative_planner_enabled is true)
  planner:
    max_plan_queries: 8     # Max queries generated in planning phase
    max_gap_queries: 3       # Max gap-fill queries from reflection phase
    reflection_enabled: false # Set false to skip reflection (saves 1 LLM call)

  # Google search via Serper API
  google_enabled: true
  google_max_results: 10

  # AskNews integration
  asknews_enabled: true
  asknews_max_results: 10

  # Agentic search (iterative LLM-guided research)
  agentic_search_enabled: true
  # Max steps per mode (use a single number for all modes, or per-mode overrides)
  agentic_search_max_steps:
    test: 3
    preview: 5
    live: 7

  # Google Trends (historical data for trends questions)
  google_trends_enabled: true

  # FRED (Federal Reserve Economic Data)
  fred_enabled: true

  # yFinance (stock/ETF prices, fundamentals, options)
  yfinance_enabled: true

  # Stock return distribution (programmatic base rate for close_price_rises questions)
  # Automatically detects stock close price questions and computes P(positive N-day return)
  # from historical yFinance data. Injected into forecaster prompts as a base rate anchor.
  stock_return_enabled: true
  stock_return_history_years: 10

  # Scrape URLs found in question text (resolution_criteria, fine_print, description)
  question_url_scraping_enabled: true
  question_url_max_scrape: 5

  # Community prediction scraping (Playwright + Xvfb)
  # Scrapes current community prediction for meta-questions
  # ("Will the community prediction be higher/lower than X%...")
  community_prediction_scraping_enabled: true
  community_prediction_wait_seconds: 5.0

  # Article scraping
  scraping_enabled: true
  max_articles_to_scrape: 10
  max_content_length: 15000

  # Legacy settings (for backward compatibility)
  sources:
    - type: "llm_knowledge"
      enabled: true

    - type: "asknews"
      enabled: true
      cache_mode: "no_cache"

    - type: "asknews_wiki"
      enabled: true
      max_results: 5

    - type: "article_scraping"
      enabled: true
      max_articles: 5
      max_content_length: 5000

# =============================================================================
# SUBMISSION CONFIGURATION
# =============================================================================
submission:
  store_reasoning: true
  tournament_id: null

# =============================================================================
# STORAGE CONFIGURATION
# =============================================================================
storage:
  base_dir: "./data"
  database_path: "./data/forecasts.db"
  snapshot_configs: true

# =============================================================================
# LOGGING CONFIGURATION
# =============================================================================
logging:
  level: "INFO"
  log_llm_calls: true
  track_costs: true

# =============================================================================
# LLM CONFIGURATION
# =============================================================================
llm:
  # Timeout for individual LLM calls (seconds)
  # Most calls complete in <2 min; this catches hung requests from OpenRouter/providers
  # If a call times out, the agent is skipped and remaining agents are aggregated
  timeout_seconds: 240  # 4 minutes

  # Maximum output tokens for LLM responses
  # Higher values allow longer responses but increase cost
  max_output_tokens: 5000

  # Temperature settings per task type (0.0 = deterministic, 1.0 = most random)
  # Utility tasks use low temps for consistency; forecasting uses moderate temps
  # since diversity already comes from model mixing + cross-pollination.
  temperature:
    query_generation: 0.3       # Focused, relevant search queries (legacy path)
    research_planning: 0.3      # Research plan generation (iterative planner)
    research_reflection: 0.2    # Coverage evaluation (iterative planner, lower = more conservative)
    article_summarization: 0.1  # Faithful extraction of article content
    agentic_search: 0.3         # Focused iterative research
    forecasting: 0.5            # Moderate diversity (model mix provides the rest)

# =============================================================================
# SUPERVISOR CONFIGURATION
# =============================================================================
# Optional supervisor agent that reviews ensemble predictions when forecasters
# significantly disagree. Based on AIA Forecaster paper (arxiv 2511.07678v1).
#
# The supervisor:
# 1. Analyzes disagreements between 5 forecasters
# 2. Conducts targeted research to resolve factual disputes
# 3. Produces an updated forecast that replaces the weighted average

supervisor:
  # Master switch - set to true to enable supervisor in the pipeline
  enabled: true

  # Model used for supervisor analysis and update
  model:
    fast: "openrouter/openai/gpt-4o-mini"
    quality: "openrouter/anthropic/claude-opus-4.6"

  # Divergence thresholds - supervisor only runs when ensemble divergence exceeds these
  divergence_threshold:
    binary: 15.0         # Std dev of probabilities in percentage points
    numeric: 0.045       # Range-normalized spread: std_dev(medians) / (range_max - range_min)
    multiple_choice: 25.0  # Max per-option range in percentage points

  # Maximum search queries the supervisor can generate
  max_search_queries: 4

  # Temperature for supervisor LLM calls
  temperature: 0.3
