# Metaculus AI Forecasting Bot Configuration
# All tunable parameters in one place

# =============================================================================
# MODEL CONFIGURATION
# =============================================================================
models:
  # Fast/cheap model for question analysis and query generation
  classifier: "claude-3-haiku-20240307"
  query_generator: "claude-3-haiku-20240307"

  # Reasoning model for outside view base rate estimation
  base_rate_estimator: "claude-sonnet-4-20250514"

# =============================================================================
# ENSEMBLE CONFIGURATION
# =============================================================================
ensemble:
  agents:
    - name: "evidence_reasoner"
      model: "claude-sonnet-4-20250514"
      weight: 1.0
      role_description: |
        You are an evidence-focused forecaster. Your job is to carefully weigh
        all available evidence, classify each piece by strength (strong, moderate, weak),
        and reason through how the evidence updates the base rate.

    - name: "contrarian"
      model: "claude-sonnet-4-20250514"
      weight: 1.0
      role_description: |
        You are a contrarian forecaster. Your job is to find counterarguments,
        identify reasons the consensus might be wrong, and consider scenarios
        that would make the base case prediction look foolish.

    - name: "calculator"
      model: "gpt-4o"
      weight: 1.5
      role_description: |
        You are a quantitative forecaster. Your job is to focus on numbers,
        statistics, and mathematical reasoning. Look for base rates,
        historical frequencies, and quantifiable trends.

    - name: "integrator"
      model: "o3"
      weight: 2.0
      role_description: |
        You are an integrating forecaster. Your job is to synthesize multiple
        perspectives, resolve conflicts between different lines of evidence,
        and produce a well-calibrated final estimate.

  # Aggregation method: "weighted_average", "median", "trimmed_mean"
  aggregation: "weighted_average"

# =============================================================================
# RESEARCH CONFIGURATION
# =============================================================================
research:
  sources:
    - type: "perplexity"
      enabled: true
      model: "sonar-reasoning-pro"
      max_queries: 3

    - type: "web_search"
      enabled: true
      provider: "serper"  # or "exa"
      max_results: 10

    - type: "asknews"
      enabled: false  # Enable if you have AskNews credentials
      max_results: 10

  # How many search queries to generate per question
  queries_per_question: 5

  # Maximum total research time per question (seconds)
  max_research_time: 60

# =============================================================================
# PROMPT CONFIGURATION
# =============================================================================
prompts:
  # Prompt template file locations (relative to src/bot/prompts/)
  outside_view_template: "outside_view.md"
  inside_view_template: "inside_view.md"
  evidence_classification: "evidence_classification.md"
  calibration_checklist: "calibration_checklist.md"

  # Verbosity level: "maximum", "balanced", "minimal"
  # maximum = full chain-of-thought, all evidence cited (~1000+ words per agent)
  # balanced = key reasoning steps, main evidence (~300-500 words per agent)
  # minimal = brief justification only (~100-150 words per agent)
  verbosity: "maximum"

# =============================================================================
# CALIBRATION CONFIGURATION
# =============================================================================
calibration:
  enabled: true
  checklist_items:
    - id: "paraphrase"
      description: "Can you summarize the question in <30 words?"
    - id: "base_rate_grounded"
      description: "Is the final prediction rooted in the outside view base rate?"
    - id: "consistency_test"
      description: "Does 'X out of 100 times, this happens' feel right?"
    - id: "evidence_audit"
      description: "Are the top 3-5 pieces of evidence factually valid?"
    - id: "blind_spots"
      description: "What scenario would make this forecast look silly?"
    - id: "status_quo_bias"
      description: "Have you appropriately considered inertia/status quo?"

# =============================================================================
# SUBMISSION CONFIGURATION
# =============================================================================
submission:
  # If true, don't actually submit to Metaculus (for testing)
  dry_run: false

  # Save full reasoning artifacts for every forecast
  store_reasoning: true

  # Skip submission if ensemble standard deviation exceeds this threshold
  # (0.0 = always submit, 0.3 = skip if agents disagree by >30%)
  max_ensemble_disagreement: 0.0

  # Metaculus tournament ID (update for each tournament)
  tournament_id: null  # Set this to the current tournament ID

# =============================================================================
# STORAGE CONFIGURATION
# =============================================================================
storage:
  # Base directory for all artifacts
  base_dir: "./data"

  # SQLite database for analytics
  database_path: "./data/forecasts.db"

  # Keep config snapshots for reproducibility
  snapshot_configs: true

# =============================================================================
# LOGGING CONFIGURATION
# =============================================================================
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR

  # Log all LLM prompts and responses
  log_llm_calls: true

  # Log API costs
  track_costs: true
