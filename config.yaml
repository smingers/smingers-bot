# Metaculus AI Forecasting Bot Configuration
# 5-Agent Ensemble Pipeline

# =============================================================================
# RUN MODE
# =============================================================================
# Controls model selection and submission behavior:
#   - "test": Cheap models (Haiku), no submission - for testing pipeline
#   - "preview": Production models, no submission - for evaluating quality
#   - "live": Production models, submits to Metaculus
#
# Override with --mode flag: python main.py --question 12345 --mode live

mode: "test"

# =============================================================================
# MODEL CONFIGURATION
# =============================================================================
# Models used for research query generation and other utility tasks.
# All models accessed via OpenRouter (set OPENROUTER_API_KEY).
#
# The 5-agent ensemble is configured separately below.

models:
  # Models for test mode (fast, inexpensive)
  fast:
    query_generator: "openrouter/anthropic/claude-3.5-haiku"
    article_summarizer: "openrouter/anthropic/claude-3.5-haiku"
    agentic_search: "openrouter/anthropic/claude-3.5-haiku"

  # Models for preview and live modes (higher quality)
  quality:
    query_generator: "openrouter/openai/o3"
    article_summarizer: "openrouter/anthropic/claude-sonnet-4.5"
    agentic_search: "openrouter/openai/o3"

# =============================================================================
# 5-AGENT ENSEMBLE CONFIGURATION
# =============================================================================
# The ensemble uses 5 forecasting agents with configurable weights and model 
# selection
#
# Cross-pollination structure (creates cross-model diversity):
#   - Agent 1 (Sonnet 4.5) receives Agent 1's step 1 output (Sonnet 4.5)
#   - Agent 2 (Sonnet 4.5) receives Agent 4's step 1 output (o3)
#   - Agent 3 (GPT-5.2) receives Agent 2's step 1 output (Sonnet 4.5)
#   - Agent 4 (o3) receives Agent 3's step 1 output (GPT-5.2)
#   - Agent 5 (o3) receives Agent 5's step 1 output (o3)

ensemble:
  # Agents for test mode (all Haiku - fast, inexpensive)
  fast:
    - name: "forecaster_1"
      model: "openrouter/anthropic/claude-3.5-haiku"
      weight: 1.0

    - name: "forecaster_2"
      model: "openrouter/anthropic/claude-3.5-haiku"
      weight: 1.0

    - name: "forecaster_3"
      model: "openrouter/anthropic/claude-3.5-haiku"
      weight: 1.0

    - name: "forecaster_4"
      model: "openrouter/anthropic/claude-3.5-haiku"
      weight: 1.0

    - name: "forecaster_5"
      model: "openrouter/anthropic/claude-3.5-haiku"
      weight: 1.0

  # Agents for preview and live modes (higher quality)
  quality:
    - name: "forecaster_1"
      model: "openrouter/anthropic/claude-sonnet-4.5"
      weight: 1.0

    - name: "forecaster_2"
      model: "openrouter/anthropic/claude-sonnet-4.5"
      weight: 1.0

    - name: "forecaster_3"
      model: "openrouter/openai/gpt-5.2"
      weight: 1.0

    - name: "forecaster_4"
      model: "openrouter/openai/o3"
      weight: 1.0

    - name: "forecaster_5"
      model: "openrouter/openai/o3"
      weight: 1.0

  aggregation: "weighted_average"

# =============================================================================
# RESEARCH CONFIGURATION
# =============================================================================
# Research is integrated into each handler. The pipeline:
# 1. Generate historical queries (outside view context)
# 2. Generate current queries (inside view context)
# 3. Execute searches via Google (Serper) and AskNews
# 4. Extract and summarize content

research:
  # Google search via Serper API
  google_enabled: true
  google_max_results: 10

  # AskNews integration
  asknews_enabled: true
  asknews_max_results: 10
  asknews_hours_back: 72

  # Agentic search (iterative LLM-guided research)
  agentic_search_enabled: true
  agentic_search_max_steps: 7

  # Google Trends (historical data for trends questions)
  google_trends_enabled: true

  # FRED (Federal Reserve Economic Data)
  fred_enabled: true

  # yFinance (stock/ETF prices, fundamentals, options)
  yfinance_enabled: true

  # Scrape URLs found in question text (resolution_criteria, fine_print, description)
  question_url_scraping_enabled: true
  question_url_max_scrape: 5

  # Article scraping
  scraping_enabled: true
  max_articles_to_scrape: 10
  max_content_length: 15000

  # Legacy settings (for backward compatibility)
  sources:
    - type: "llm_knowledge"
      enabled: true

    - type: "asknews"
      enabled: true
      cache_mode: "no_cache"

    - type: "asknews_wiki"
      enabled: true
      max_results: 5

    - type: "article_scraping"
      enabled: true
      max_articles: 5
      max_content_length: 5000

# =============================================================================
# SUBMISSION CONFIGURATION
# =============================================================================
submission:
  store_reasoning: true
  tournament_id: null

# =============================================================================
# STORAGE CONFIGURATION
# =============================================================================
storage:
  base_dir: "./data"
  database_path: "./data/forecasts.db"
  snapshot_configs: true

# =============================================================================
# LOGGING CONFIGURATION
# =============================================================================
logging:
  level: "INFO"
  log_llm_calls: true
  track_costs: true

# =============================================================================
# LLM CONFIGURATION
# =============================================================================
llm:
  # Timeout for individual LLM calls (seconds)
  # Most calls complete in <2 min; this catches hung requests from OpenRouter/providers
  # If a call times out, the agent is skipped and remaining agents are aggregated
  timeout_seconds: 240  # 4 minutes

  # Maximum output tokens for LLM responses
  # Higher values allow longer responses but increase cost
  max_output_tokens: 5000

  # Temperature settings per task type (0.0 = deterministic, 1.0 = most random)
  # Utility tasks use low temps for consistency; forecasting uses moderate temps
  # since diversity already comes from model mixing + cross-pollination.
  temperature:
    query_generation: 0.3       # Focused, relevant search queries
    article_summarization: 0.1  # Faithful extraction of article content
    agentic_search: 0.3         # Focused iterative research
    forecasting: 0.5            # Moderate diversity (model mix provides the rest)
