
You are currently analyzing a forecasting question to generate an outside view prediction.

The forecasting question is:
What will be the Chinese model share on Openrouter for the week of April 19, 2026?

Question background:
[OpenRouter](https://openrouter.ai/) is a popular service that unifies AI APIs, allowing users to access and easily switch between a large variety of models in exchange for a percentage fee on top of regular API costs.

The US and China have the world's most advanced AI industries and are often thought to be in a geopolitical struggle or "race" to develop powerful AI systems. Chinese models have improved rapidly over the past few years.&#x20;

The release of Deepseek R1 in January 2025 attracted substantial attention for attaining close to state of the art performance with a much lower training cost. Since then, Chinese models have continued to focus on efficiency and latency over pure performance.

`{"format":"metac_reveal_and_close_in_period","info":{"post_id":41889,"question_id":41627}}`

This question's outcome will be determined by the specific criteria below. These criteria have not yet been satisfied:
This question will resolve as the total usage share of Chinese models on [OpenRouter](https://openrouter.ai/rankings) for the week of April 19, 2026.

This question will resolve based upon the Market Share graph section that shows per-author ranking, by summing the token count of Chinese model authors and dividing by the total token count excluding "others".

Additional fine-print:


Units for answer: %

Question metadata:
- Opened for forecasting: 2026-02-16T15:30:00Z
- Resolves: 2026-04-27T01:00:00Z
- Note: Unless the question title specifies otherwise, the Forecast Opening Date of 2026-02-16T15:30:00Z should be considered the start of the question's resolution window. Events before this date do not count toward resolution.

IMPORTANT: Today's date is 2026-02-16. All dates before today's date are in the PAST. All dates after today's date are in the FUTURE. Use today's date to correctly evaluate whether sources describe past events or future predictions. Any information source which refers to events before today's date of 2026-02-16 should not be considered as speculative but rather an historical document.

The lower bound is 15 and the upper bound is 45.
Both bounds are OPEN: outcomes can fall below the lower bound or above the upper bound. Your percentile estimates may extend beyond this range if well-supported by evidence.

Historical context:

<QuestionSource url="https://openrouter.ai/rankings">
# Summary of OpenRouter LLM Rankings Page

**Disclaimer:** The extracted content appears to be a navigation/header section of the OpenRouter rankings page rather than the full article content with detailed statistics and data.

## Key Information:

**Source Description:**
- OpenRouter provides LLM rankings based on real usage data from millions of users accessing models through their platform

**Available Metrics/Comparisons:**
The page offers the following types of data visualization and comparison:
1. Weekly usage of models across OpenRouter
2. Most popular models on OpenRouter
3. **Token share by model author** (directly relevant to the forecasting question)
4. Models by use case
5. Models by natural language
6. Models by programming language
7. Requests by prompt & completion length
8. Tool usage across models
9. Total images processed
10. Largest public apps opting into usage tracking

**Relevant to Question:**
The page specifically mentions tracking "Open Router token share by model author," which is the exact metric needed to calculate Chinese model share (by summing token counts from Chinese model authors and dividing by total token count excluding "others").

**Note:** The extracted content does not contain actual numerical data, statistics, or current market share figures - it only describes what types of data are available on the OpenRouter rankings page. To answer the forecasting question, one would need to access the actual interactive graphs and data on the live website.
</QuestionSource>

<QuestionSource url="https://openrouter.ai/">
# Summary of OpenRouter Article

**Disclaimer:** This appears to be marketing/landing page content from OpenRouter's website rather than a news article or analytical piece. The content is limited and does not contain information directly relevant to forecasting Chinese model market share.

## Key Information Extracted:

### Facts and Statistics:
- OpenRouter has 250,000+ apps using the service
- 4.2 million+ users globally
- The platform provides access to AI models from "all major labs and providers"

### Service Description:
- OpenRouter is a unified interface for accessing multiple AI models
- Compatible with OpenAI SDK
- Features distributed infrastructure with fallback capabilities when providers go down
- Runs at the edge for minimal latency
- Offers fine-grained data policies for organizational control

### Recent Announcements Mentioned:
- Launch of "Response Healing" feature (automatically fixes malformed JSON responses from LLMs)
- 2025 State of AI report in partnership with a16z (described as "the largest empirical look yet at how developers and organizations use language models in the real world")
- Content about "license-safe synthetic data workflows for model specialization"

**Note:** This content does not provide specific information about Chinese model market share, usage statistics by model origin, or trends that would directly inform the forecasting question. To answer the question about Chinese model share for April 19, 2026, one would need to access OpenRouter's actual rankings/market share data page referenced in the question's resolution criteria.
</QuestionSource>


<Summary source="https://www.interconnects.ai/p/latest-open-artifacts-12-qwen3-235b-a22b-instruct-2507">
# Summary of "Latest open artifacts (#12): Chinese models continue to dominate throughout the summer"

**Source:** Interconnects.ai  
**Author:** Nathan Lambert  
**Date:** Not explicitly stated, but references events "throughout the summer" and mentions a release "earlier today"

## Key Facts and Observations:

### Market Presence and Strategy:
- Chinese companies have been actively growing their presence on X (Twitter) to increase U.S. market share since February
- Nathan Lambert personally received DMs from 3 leading Chinese frontier model laboratories on Twitter requesting collaboration/promotion (zero from Western companies)
- Small subdivisions of Alibaba like Tongyi are expanding their presence
- Qwen launched a new page (Qwen.ai) for model tinkering, similar to Google's AI Studio

### Kimi K2 Case Study:
- Days before launch, Kimi messaged several people in the AI space (including smaller accounts) offering pre-release access
- Kimi became the only provider (to their knowledge) offering an Anthropic-compatible API
- K2 became "one of the most used models on Open Router"
- Matched Opus on LMArena performance

### Overall Quality Trend:
- This issue of Artifacts Log described as "the most impressive yet" in terms of overall quality
- A year ago, content was "a mix of half-baked research artifacts and interesting ideas"
- Today, "there are viable open models for many real-world tasks"

### Base Model Dominance Shift:
- Historical data shows Meta's Llama originally dominated in Artifacts Log
- Qwen has been "the default for many months" now

### Specific Model Releases Mentioned:

**Qwen 3-235B-A22B-Instruct-2507:**
- Scored 41.8 on ARC-AGI
- Beat Kimi K2 model
- Received "multiple reports of strong vibe tests"
- Qwen historically known for "benchmark-maximizing" with some papers highlighting "signs of data contamination in Qwen base models"
- Models improving in "robustness of normal testing"

**Multiple other Chinese models listed:** Hunyuan-A13B-Instruct by Tencent, ERNIE-4.5-21B-A3B-PT by Baidu, pangu-pro-moe-model by Huawei Pangu team

### Author's Assessment:
Quote: "Chinese labs and companies continue to outbid each other in the open model space with very solid models"

**Note:** The article appears to be cut off mid-sentence at the end, so the full content may not be captured here.
</Summary>

<Summary source="https://openrouter.ai/state-of-ai">
# Summary of "State of AI 2025: 100T Token LLM Usage Study | OpenRouter"

## Key Facts and Statistics:

1. **Dataset Scale**: The study analyzes over 100 trillion tokens of real-world LLM interactions on the OpenRouter platform, spanning approximately two years up to the time of writing (2025).

2. **Platform Details**: 
   - OpenRouter supports 300+ active models from over 60 providers
   - Serves millions of developers and end-users
   - Over 50% of usage originates outside the United States

3. **Historical Context**: 
   - December 5, 2024: OpenAI released the first full version of its o1 reasoning model (preview was September 12, 2024)
   - This marked a shift from single-pass pattern generation to multi-step deliberation inference

4. **Methodology**:
   - Analysis based on anonymized request-level metadata (no access to actual prompt/completion text)
   - Metadata includes timing, model identifiers, token usage (prompt and completion), geographic routing, latency, and usage context
   - Task categorization performed on ~0.25% random sample of prompts/responses using Google Tag Classifier (Google Cloud Natural Language's classifyText API)
   - All analyses conducted using Hex analytics platform

## Key Findings Mentioned (but not detailed in excerpt):

The study observes:
- Substantial adoption of open-weight models
- Outsized popularity of creative roleplay and coding assistance categories (beyond just productivity tasks)
- Rise of agentic inference
- "Cinderella Glass Slipper effect" - early users show longer engagement persistence than later cohorts

## Limitations:

- Dataset is observational and reflects activity specific to OpenRouter platform
- Usage patterns shaped by model availability, pricing, and user preferences on the platform
- Task categorization based on only 0.25% sample of total activity

**Note**: The article excerpt appears to be incomplete, cutting off mid-sentence during the methodology section. The full findings and detailed analysis sections are not included in this extract.
</Summary>

<Summary source="https://www.aljazeera.com/economy/2025/11/13/chinas-ai-is-quietly-making-big-inroads-in-silicon-valley">
# Summary of "China's AI is quietly making big inroads in Silicon Valley"

**Source:** Al Jazeera  
**Author:** John Power

## Key Facts and Statistics:

1. **OpenRouter Usage Data (last week, relative to article publication):**
   - Chinese AI tools took 7 spots among the top 20 models with most usage
   - Specific Chinese models mentioned in top 20: MiniMax's M2, Z.ai's GLM 4.6, and DeepSeek's V3.2
   - Among top 10 models used for programming, 4 were developed by Chinese firms

2. **Competitive Advantages of Chinese Models:**
   - Chinese developers offer "open" language models at much lower costs than US rivals
   - Use older-generation chips not subject to US export controls
   - Use relatively small quantities of chips, dramatically reducing training and running costs
   - Lower input costs and more compute-efficient models enable cheaper hosting services

## Named Source Opinions:

1. **Brian Chesky (Airbnb CEO):** Revealed in October that Airbnb chose Alibaba's Qwen over OpenAI's ChatGPT, praising it as "fast and cheap"

2. **Chamath Palihapitiya (Social Capital CEO):** Revealed in October that his company migrated much of its work to Moonshot's Kimi K2 because it was "way more performant" and "a ton cheaper" than OpenAI and Anthropic models

3. **Nathan Lambert (Machine Learning Researcher, Atom Project founder):** 
   - Called public examples the "tip of the iceberg"
   - Stated "Chinese open models have become a de facto standard among startups in the US"
   - Noted many US firms are reluctant to publicly disclose their use of Chinese technology

4. **Rui Ma (Tech Buzz China founder):**
   - Chinese models particularly attractive to fledgling startups
   - "High-resource organisations" gravitate towards premium US models
   - Users are "typically cost-conscious early-stage companies that experiment widely, and many of them will not survive"
   - Suggested AI adoption might follow mobile platform dynamics (Android vs iPhone), where affordability drives broader adoption but premium options retain value concentration

5. **Toby Walsh (AI Expert, University of New South Wales):**
   - "The success of these Chinese models demonstrates the failure of export controls to limit China"
   - Export controls "actually encouraged Chinese companies to be more resourceful and build better models that are smaller and are trained on and run on older generation hardware"

6. **Greg Slabaugh (Professor, Queen Mary University of London):**
   - "China's AI progress has been underestimated, partly because the signal is fragmented"
   - "Much of the uptake of Chinese models is in China"
   - In Fortune 500 and regulated sectors, "widespread adoption is probably not imminent"
   - Any "rude awakening" may come on pricing and flexibility rather than sudden displacement of US models

## Additional Context:

- Programmers on social media highlighted evidence that two popular US-developed coding assistants (Composer and Windsurf) were built on Chinese models
- Their developers (Cursor and Cognition AI) have not publicly confirmed use of Chinese technology
- Z.ai stated the speculation aligns with its "internal findings"
- Chinese developers mentioned: Alibaba, Z.ai, Moonshot, MiniMax, DeepSeek
- Chinese models use "open-weight" approach, making trained parameters publicly available (unlike leading US platforms like ChatGPT)
- Analysts suggest US tech giants remain well-positioned to dominate high-end market and highly regulated sectors where national security is paramount
</Summary>

<Summary source="https://www.nbcnews.com/tech/innovation/silicon-valley-building-free-chinese-ai-rcna242430">
# Summary of "More of Silicon Valley is building on free Chinese AI"

**Source:** NBC News  
**Date:** November 30, 2025

## Key Facts and Statistics:

1. **Reflection AI valuation:** Misha Laskin (former Google ML engineer) founded Reflection AI to provide open-source American alternatives to Chinese models. The startup is valued at $8 billion.

2. **Exa company details:** AI-focused search company valued at $700 million, backed by Lightspeed Venture Partners and Nvidia.

3. **Dayflow usage statistics:** Approximately 40% of Dayflow's users now choose to use open-source models. The app's founder Jerry Liu estimates that paying for users' closed-model usage can cost up to $1,000 per person.

4. **Kilo Code model distribution:** Of the 20 top models among Kilo Code users, seven are Chinese models, with six of those seven being open-source.

5. **Release pace comparison:** Alibaba released a new model roughly every 20 days in 2025, compared with Anthropic's 47-day average between releases.

## Named Source Opinions:

1. **Misha Laskin (Reflection AI founder):** Stated that Chinese open models are "surprisingly close to the frontier" and "You're starting to see glimpses of open-model companies actually driving the frontier of intelligence in China."

2. **Michael Fine (Head ML at Exa):** Said running Chinese models on their own hardware has proved "significantly faster and less expensive" than using bigger models like OpenAI's GPT-5 or Google's Gemini in many cases.

3. **Lin Qiao (CEO of Fireworks AI, co-creator of PyTorch):** Said "The gap is really shrinking" regarding capability differences between American closed-source and Chinese open-source models.

4. **Jerry Liu (Dayflow founder):** Stated "Qwen is as good as GPT-5 for my use case" and expressed privacy concerns about cloud-based processing.

5. **Brian Chesky (Airbnb CEO):** According to the article, Airbnb "heavily" relies on Chinese models like Qwen.

6. **Charles Zedlewski (CPO at Together AI):** Said developers now find it "simpler and more efficient to start from open models and adapt them with their own data."

7. **Nathan Lambert (Senior Research Scientist at Allen Institute for AI):** Called Chinese developers "genuine innovators in AI" and stated "The balance of power has been shifting rapidly in the last 12 months."

## Key Trends Identified:

- Chinese open-source AI models are increasingly being adopted by American AI startups as alternatives to expensive U.S. closed models
- Chinese models offer advantages in cost, customization, speed, and privacy (on-device processing)
- Performance gap between Chinese open-source and American closed-source models has narrowed significantly
- Chinese models dominate developer resources and community support online
- Platforms like OpenRouter are seeing users gravitate toward Chinese open-source models
- China's government actively supports open-source AI development (Xi Jinping's Nov. 1 economic address called for greater "cooperation on open-source technologies")

**Note:** The article text appears to be cut off at the end ("Some in Silicon Va"), so the summary may be missing concluding points.
</Summary>

<Summary source="https://arynews.tv/chinese-open-source-ai-models-capture-nearly-30-of-global-usage">
# Summary of Article: "Chinese open-source AI models capture nearly 30% of global usage"

**Source:** Arynews.tv  
**Author:** Kumail Shah  
**Date:** December 10, 2025

## Key Facts and Statistics:

1. **Chinese Model Market Share:**
   - Chinese open-source AI models accounted for nearly 30% of total global usage as of the report's publication
   - Chinese open-source LLMs' global share started from 1.2% in late 2024 and reached nearly 30% over a few months in 2025
   - Chinese AI models accounted for an average of 13% of weekly token volume in 2025
   - Growth accelerated in the latter half of 2025, bringing their average usage close to 13.7% (the share recorded by AI models from the rest of the world)

2. **Chat Models Dominance:**
   - Chat models (such as OpenAI's GPT-4o and GPT-5) remained dominant with a 70% global share

3. **Language Usage:**
   - Chinese became the second most used prompt language worldwide, accounting for nearly 5% of all requests (behind English)
   - This was significantly higher than Chinese language's share on the internet, which stood at about 1.1%

4. **Geographic Distribution:**
   - China ranked fourth behind the US, Singapore, and Germany in terms of global share in LLM tokens

5. **Market Fragmentation:**
   - By late 2025, the market had become fragmented with no single model surpassing a 25% share

## Source Information:

- **Report Source:** Open Router (a third-party AI model aggregator) and venture capital firm Andreessen Horowitz
- **Study Scope:** Empirical study of 100 trillion tokens by Open Router

## Key Models Mentioned:

- Qwen family from Alibaba Group Holding
- DeepSeek's V3
- Moonshot AI's Kimi K2

## Notable Observations from the Report:

- The report stated that "China has emerged as a major force, not only through domestic consumption but also by producing globally competitive models"
- The ascent reflects "not only competitive quality but also rapid iteration and dense release cycles"
- Chinese open-source models gained recognition for their increased efficiency and cost-effectiveness
</Summary>

<Summary source="https://www.scmp.com/tech/tech-trends/article/3335602/chinas-open-source-models-make-30-global-ai-usage-led-qwen-and-deepseek">
# Summary of Article: "Chinese open-source models account for 30% of global AI use: report"

**Source:** South China Morning Post  
**Date:** December 08, 2025

## Key Facts and Statistics:

1. **Chinese open-source LLM market share trajectory (according to Open Router empirical study of 100 trillion tokens):**
   - Late 2024: Started at 1.2% global share
   - Reached nearly 30% "over a few months" in 2025
   - 2025 year-to-date average: 13% of weekly token volume

2. **Comparative market share:**
   - Chinese open-source LLMs: 13% average weekly token volume (2025 YTD)
   - AI models from "rest of the world": 13.7% average (2025 YTD)
   - Growth accelerated in the second half of 2025

## Key Opinions:

**From the report (source not further specified):**
- "China has emerged as a major force, not only through domestic consumption but also by producing globally competitive models"
- The report characterizes producing globally competitive models as making China "a close peer of the US in AI development"

## Note:
The article references an "empirical study" and "report" by Open Router but does not provide additional details about the methodology or exact publication date of this report beyond the article's December 2025 date.
</Summary>

<Agent_report>
Query: List Chinese author token share OpenRouter weekly market share history; include Qwen Deepseek Baichuan Moonshot Yi, cached rankings from 2025-2026
Objective of the research step  
• Produce a historical list of OpenRouter weekly market-share (token share) for the Chinese model authors Qwen, DeepSeek, Baichuan, Moonshot and Yi, covering every cached week that can be recovered from 2025-2026.  
• This list will later be used to extrapolate Chinese share for the resolution week of 19 Apr 2026.

What we were able to find in the current search round  
1. Live OpenRouter “LLM Rankings” page (openrouter.ai/rankings) – shows the current week’s author-level token shares but not historical values. No CSV export. The page uses a client-side JSON request that is *not* captured by the Wayback Machine during most crawls.  
2. 11 Nov 2025 PDF on Scribd entitled “LLM Rankings – OpenRouter” (source [1]). It reproduces the author graph for that specific week. The PDF lists individual author shares (Grok, Anthropic, OpenAI, Qwen, DeepSeek, etc.). Qwen = 8.1 %; DeepSeek = 4.7 %; Moonshot = 1.3 %; Baichuan = 0.4 %; Yi (01.AI) = 0.2 %. Total Chinese = 14.7 % that week.  
3. South China Morning Post article (8 Dec 2025) summarising the “100 T Token Study” by OpenRouter (source [2]). It states that Chinese *open-source* models averaged 13 % of weekly tokens for “so far this year” and reached “nearly 30 %” in some H2-2025 weeks. The article, however, aggregates all Chinese authors together and does not break out our five individual labs.  
4. “State of AI 2025: 100 T Token LLM Usage Study” (OpenRouter site & arXiv mirrors) (sources [3],[4]). This contains several line-charts in the appendix, including a stacked-area plot entitled “Weekly token share by author (Jan 2024 – Oct 2025)”. The legend explicitly shows Qwen, DeepSeek, Baichuan, Moonshot, Yi and other Chinese authors. Unfortunately the PDF images are rasterised; individual weekly percentages are not numerically tabulated. We can manually read a handful of data points:  
   • Week of 05 Jan 2025: all Chinese authors combined ≈ 5 %.  
   • Week of 14 Jul 2025: peak ≈ 28 % with Qwen ≈ 14 %, DeepSeek ≈ 8 %, Moonshot ≈ 3 %, Baichuan ≈ 2 %, Yi ≈ 1 %.  
   • Week of 27 Oct 2025: Chinese ≈ 18 %. These values are approximate; the plot does not give exact numbers.  
5. TeamDay.ai blog post “OpenRouter Models Ranked … (2026)” (source [5]) – only qualitative; no token-share data.  
6. UnderstandingAI / Interconnects pieces – contextual only; no usage numbers.  
7. No 2026-dated Wayback snapshots surfaced yet that include the author-share JSON; attempts in prior steps were unsuccessful.

What we can now tabulate with confidence  
From (1) live page – This week’s values (early Feb 2026) can be scraped manually; but the query asks for 2025-2026 cache, so we should capture it.  
From (1)+(2)+(3) we have three trustworthy numeric weeks:

Week-ending | Qwen | DeepSeek | Baichuan | Moonshot | Yi | Total CN
----------- | -----|---------|----------|----------|----|----------
05 Jan 2025 | ~2 % | ~1.5 %  | ~0.7 %  | ~0.5 %  | ~0.3 % | ~5 %
14 Jul 2025 | 14 % | 8 %     | 2 %     | 3 %     | 1 % | ~28 %
11 Nov 2025 | 8.1 %| 4.7 %   | 0.4 %   | 1.3 %   | 0.2 %| 14.7 %

Gaps that remain  
• For almost every other week we still lack exact numbers. The resolution question (Apr 19 2026) requires the precise “token count by author” for that week, not an estimate. We therefore need:  
   – Weekly JSON dumps or screenshots for every week between Nov 2025 and Apr 2026 (at minimum Jan-Apr 2026).  
   – Earlier 2025 weeks (Feb-Jun, Aug-Oct) to improve trend analysis.  
• Wayback does hold a *few* snapshots with the author-share JSON but they are not returned in generic Google search. They must be located by querying specific timestamps or by using the CDX API for the endpoint https://openrouter.ai/api/usage/rankings?resolution=weekly&period=author .  
• Some crypto-newsletters occasionally copy-paste the weekly OpenRouter leaderboard on Monday mornings; these could fill gaps (Substack, X screenshots).

Recommended next research actions (new queries)  
1. Use Wayback CDX: site:openrouter.ai/api/usage “period=author” weekly (Google) – to list all archived JSON endpoints.  
2. “OpenRouter rankings week of” (Google) – then iterate with specific dates “18 Aug 2025”, “1 Feb 2026” etc.  
3. “OpenRouter token share screenshot” (Google News) – look for social-media images.  
4. “github openrouter weekly author json” (Google) – some developers store the dumps in repos.  
5. site:substack.com “OpenRouter rankings” (Google) – newsletters that regularly log the chart.

Until we extract those additional datapoints, the historical record is still incomplete for forecasting.</Agent_report>


The information has been sourced from the internet and language models (for agent reports). Exercise healthy skepticism toward unverified claims.

Your analysis should have the following components, referring to the above historical context:
(a) Source analysis: Briefly summarize each information source (either web article or Agent report), evaluate source quality and date.
**Opinions are commonplace in writing. For each source, you must be able to discern factual information from opinions. You are advised to strongly consider only opinions originating from identifiable experts or entities**.
(b) Reference class analysis: Identify a few possible reference classes and evaluate respective suitabilities to the forecasting question. If applicable, choose the most suitable one.
(c) Timeframe analysis: State the prediction timeframe (e.g., how many days/months from now?) and examine historical patterns over similar periods
(d) Justification: Integrate the above factors with other points you found relevant to write a justification for your outside view prediction.

Subsequently, calibrate your outside view prediction, considering:
(a) You aim to predict a true probability distribution, not a hedged smooth distribution or an overconfident extremely narrow distribution. In your thinking, always consider ranges over singular values.
(b) Are there previously established distributions that you can tether your prediction to?
(c) Small changes in percentile values can significantly reshape the distribution, especially near the tails. Choose tail values carefully.
(d) Historically, what is the rate of upsets/unexpected outcomes in the domain of this forecasting question? How should this affect your CDF distribution?

Set wide 10th/90th percentile intervals to account for unknown unknowns.

**CRITICAL: Percentile values MUST be strictly increasing** (10th = lowest, 90th = highest).
Use the units requested by the question. Never use scientific notation.

Format your answer as below:

Analysis:
{Insert your analysis here, following the above components.}

Outside view calibration:
{Insert your calibration of your outside view prediction here.}

Outside View Prediction:
Percentile 10: XX
Percentile 20: XX
Percentile 40: XX
Percentile 60: XX
Percentile 80: XX
Percentile 90: XX
