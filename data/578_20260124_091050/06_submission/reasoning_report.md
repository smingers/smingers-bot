# Forecast: Will humans go extinct before 2100?
**Question ID:** 578
**Submitted:** 2026-01-24 09:11:54 UTC
**Type:** binary
**Final Prediction:** 4.0%

## Question Summary


**Resolution:** 

## Research Summary
- Consulted 28 sources

See research artifacts

## Outside View (Base Rate)

**Reference classes considered:**
1. Large Mammalian Species Extinctions**
2. Hominin Species Extinctions**
3. Civilization Collapse Events**
4. Mass Extinction Events**

**Base rate estimate:** 1.0%

**Reasoning:**
# Base Rate Analysis: Human Extinction Before 2100

## Step 1: Source Analysis

The historical data for human extinction events is fundamentally limited because:
- **Sample size of 1**: Homo sapiens has never gone extinct (by definition, since we're here)
- **Observational bias**: We can only observe timelines where humanity survived to the present
- **Limited timeframe**: Modern civilization spans only ~10,000 years; industrial civilization ~250 years
- **Data quality**: Archaeological records become sparse going back in time
- **Definitional challenges**: What constitutes "human extinction" vs. evolutionary transition is debated

Available data sources include:
- Paleontological records of species extinctions
- Archaeological evidence of civilization collapses
- Historical records of population bottlenecks
- Geological records of mass extinction events

## Step 2: Reference Class Analysis

**Reference Class 1: Large Mammalian Species Extinctions**
- Historical base rate: ~1-2% per century for large mammals in recent millennia
- Sample size: Thousands of large mammal species over geological time
- Suitability score: 3/10
- Why this reference class: Humans are large mammals, but our technology, global distribution, and adaptability make us fundamentally different from other species. This severely limits applicability.

**Reference Class 2: Hominin Species Extinctions**
- Historical base rate: ~100% eventually (all previous hominin species extinct)
- Sample size: ~20-30 hominin species over 7 million years
- Suitability score: 4/10
- Why this reference class: More relevant as close evolutionary relatives, but previous hominins lacked modern technology, global civilization, and population sizes. Timeline also much longer than our 76-year window.

**Reference Class 3: Civilization Collapse Events**
- Historical base rate: ~5-10% of major civilizations experience total collapse per century
- Sample size: ~20-30 major historical civilizations
- Suitability score: 2/10
- Why this reference class: Civilization collapse ≠ species extinction. Even major collapses (Roman Empire, Maya, etc.) didn't eliminate the underlying populations, just political/social structures.

**Reference Class 4: Mass Extinction Events**
- Historical base rate: ~1 per 10-20 million years (so ~0.000001% per century)
- Sample size: 5 major mass extinctions in 540 million years
- Suitability score: 5/10
- Why this reference class: These events eliminated large percentages of species, but the timescales are vastly different and causes were typically gradual geological processes or rare cosmic events.

## Step 3: Fermi Estimation

Given poor reference classes, decomposing by extinction mechanisms:

P(extinction by 2100) = P(natural disaster) + P(human-caused) - P(overlap)

**Natural disasters:**
- P(asteroid/comet impact) ≈ 0.01% (based on impact frequency)
- P(supervolcanic eruption) ≈ 0.1% (Yellowstone-scale events)
- P(gamma ray burst) ≈ 0.001% (very rare, uncertain lethality)
- Natural subtotal ≈ 0.11%

**Human-caused (harder to estimate from base rates alone):**
- Historical precedent extremely limited for global-scale human-caused extinction
- No reference class exists for technological civilization destroying itself completely
- Estimate: Highly uncertain, 0.1-10% range based on civilization fragility

**Combined rough estimate: 0.2-10%**

## Step 4: Base Rate Integration

Weighting by suitability scores:
- Mass extinction events (weight 5): ~0.000001%
- Hominin extinctions (weight 4): Not applicable to 76-year timeframe
- Large mammals (weight 3): ~1-2%
- Civilization collapse (weight 2): Not equivalent to extinction

The mass extinction reference class suggests extremely low probability, but may miss human-specific risks. The mammalian extinction rate is likely too high given human advantages. Fermi estimation suggests 0.2-10% range.

**Integrated estimate: ~1%**

## Step 5: Confidence Assessment

Confidence: **2/10** - Very uncertain

The fundamental problem is lack of relevant historical precedent for:
- Technological species extinction
- Global civilization collapse leading to species extinction
- Human-level intelligence going extinct in short timeframes

## Output Format

**Base Rate Estimate:** 1%

**Reference Classes Used:**
1. Mass extinction events: ~0.000001% base rate (weight: 5)
2. Hominin species extinctions: Not applicable to timeframe (weight: 4)  
3. Large mammalian extinctions: ~1-2% per century (weight: 3)
4. Civilization collapses: ~5-10% per century but ≠ extinction (weight: 2)

**Confidence Level:** 2/10

**Key Uncertainties:**
- No historical precedent for technological civilization extinction
- Survivorship bias in observable timelines
- Unclear definition boundaries between extinction vs. transformation
- Limited sample size for relevant reference classes
- Difficulty separating natural vs. anthropogenic risks using historical data alone

## Inside View (Adjustments)

### analyst (claude-sonnet-4-20250514)
**Prediction:** 4.3%
**Weight:** 1.0

**Reasoning:**
## Step 1: Source Analysis

The research provides a mix of recent and credible sources, though with notable limitations:

**Recency**: Most relevant information is from 2019-2025, with some AI extinction warnings from 2024-2025 being particularly recent.

**Credibility**: Sources include high-quality academic journals (Nature, Science), established institutions (Stanford, MIT, RAND), and reputable news outlets. However, some sensationalized headlines (Daily Express claiming "99.9% extinction risk") suggest potential media bias.

**Potential Biases**: Clear selection bias toward extinction-related content due to search terms. AI extinction warnings may reflect researcher advocacy rather than objective assessment. Media tends to amplify dramatic predictions.

**Base Rate Integration**: The 1.0% base rate appears to focus on historical extinction patterns for large mammals/hominins, but may not fully incorporate novel 21st century risks like advanced AI, engineered pandemics, or cascading climate effects.

## Step 2: Evidence Classification

**STRONG Evidence**
- **AI Development Acceleration**: Multiple credible sources (RAND, academic surveys) indicate AI researchers themselves assign significant extinction probability, with some predicting AI autonomy by 2027
- **Unprecedented Extinction Rates**: Science journal confirms current extinction rates are "unprecedented in human history and highly unusual in Earth's history"

**MODERATE Evidence**
- **Expert Survey Data**: 2008 Future of Humanity Institute survey estimated 5% extinction probability by superintelligence by 2100, though dated
- **Cascading Environmental Pressures**: UNEP data showing 165 billion tonnes of raw material extraction by 2050 indicates unprecedented planetary stress
- **Nature Paper Background Rate**: Establishes upper bounds for natural human extinction rates at 6.9 × 10^-5, suggesting current risks exceed natural baselines

**WEAK Evidence**
- **Sensationalized AI Predictions**: Individual researchers claiming "99.9% extinction risk" lack peer review and appear outlier positions
- **General Biodiversity Crisis**: While concerning, current biodiversity loss doesn't directly translate to human extinction risk

## Step 3: Direction of Update

**AI Development Acceleration**: UP by approximately 1.5 percentage points. The convergence of multiple AI researchers assigning significant extinction probability, combined with accelerating development timelines, represents a novel risk not captured in historical base rates.

**Unprecedented Current Extinction Context**: UP by approximately 0.5 percentage points. While focused on other species, the documented acceleration of extinction rates suggests we're in a fundamentally different risk environment than historical periods.

**Expert Survey Convergence**: UP by approximately 1.0 percentage points. The 2008 FHI survey's 5% estimate, while dated, represents systematic expert judgment and suggests the 1.0% base rate may be conservative for novel technological risks.

**Environmental Cascade Risks**: UP by approximately 0.3 percentage points. Unprecedented resource extraction rates create potential for cascading failures not captured in historical extinction patterns.

**Natural Background Rates**: NEUTRAL. The Nature paper's background rate analysis helps validate that we're discussing risks above natural baselines, but doesn't independently adjust probability.

## Step 4: Integration

Starting from 1.0%:
- Adjustment 1: +1.5% for AI development risks exceeding historical precedent
- Adjustment 2: +1.0% for expert survey data suggesting higher baseline risk
- Adjustment 3: +0.5% for unprecedented extinction rate context
- Adjustment 4: +0.3% for environmental cascade potential
- **Cumulative adjustment:** +3.3%

## Step 5: Sanity Checks

1. **Extremity check**: At 4.3%, this remains a low-probability event. To be wrong, either: (a) AI development would need to pose minimal risk despite expert concerns, or (b) human adaptability would need to exceed all historical precedent.

2. **Base rate anchor**: The 3.3 percentage point increase is substantial but justified by novel risk categories (advanced AI, engineered biology) not present in historical reference classes.

3. **Update magnitude**: Updates are proportional - largest adjustment (1.5%) corresponds to strongest evidence (AI researcher consensus on novel risk).

4. **Regression to mean**: The adjustment acknowledges uncertainty while avoiding extreme positions. The 4.3% estimate remains closer to conservative base rates than alarmist predictions.

## Step 6: Final Reasoning

As a geopolitical analyst, I focus on how current global dynamics create extinction risks beyond historical precedent. Three key factors drive my upward adjustment:

**Novel Technological Risks**: The AI development trajectory represents genuinely unprecedented risk. Unlike historical extinction threats, advanced AI could emerge rapidly with global reach. The fact that AI researchers themselves assign significant extinction probability (5%+ in surveys) suggests this isn't captured in historical base rates derived from natural or conventional technological risks.

**Accelerating Risk Environment**: We're operating in a fundamentally different risk landscape than historical periods. Current extinction rates for other species are "unprecedented in human history," suggesting our base rate derived from historical hominin extinctions may underestimate current environmental pressures.

**Systemic Interconnectedness**: Modern civilization's global interconnectedness means localized catastrophes can cascade globally in ways not captured by historical reference classes. Resource extraction at 165 billion tonnes annually by 2050 represents unprecedented planetary stress.

However, I maintain relative conservatism because: (1) humans have demonstrated remarkable adaptability, (2) growing awareness of existential risks is driving mitigation efforts, and (3) the most extreme predictions lack rigorous peer review.

**Evidence Summary:**
- Strong evidence pointing UP: AI researcher extinction probability assessments, unprecedented current extinction rate context
- Strong evidence pointing DOWN: None identified in current research
- Net direction: UP

**Adjustment Calculation:**
- Base rate: 1.0%
- Total adjustment: +3.3%
- Final estimate: 4.3%

**Confidence in Adjustment:** 7/10

**Probability: 4.3%**

**Evidence weighted:**
- Strong: **AI Development Acceleration**: Multiple credible sources (RAND, academic surveys) indicate AI rese, **Unprecedented Extinction Rates**: Science journal confirms current extinction rates are "unprecede
- Moderate: **Expert Survey Data**: 2008 Future of Humanity Institute survey estimated 5% extinction probability, **Cascading Environmental Pressures**: UNEP data showing 165 billion tonnes of raw material extracti, **Nature Paper Background Rate**: Establishes upper bounds for natural human extinction rates at 6.9
- Weak: **Sensationalized AI Predictions**: Individual researchers claiming "99.9% extinction risk" lack pee, **General Biodiversity Crisis**: While concerning, current biodiversity loss doesn't directly transl

### historian (claude-sonnet-4-20250514)
**Prediction:** 1.2%
**Weight:** 1.0

**Reasoning:**
## Step 1: Source Analysis

The evidence spans from 2019-2025, with most sources being recent (2023-2025). Sources include reputable academic journals (Nature, Science), established institutions (Stanford, MIT, RAND), and international organizations. However, there's a notable bias toward AI extinction risks in recent coverage, which may reflect current media attention rather than historical patterns. Most historical extinction data appears well-established and likely already incorporated into base rates, while AI risk assessments represent newer concerns.

## Step 2: Evidence Classification

**STRONG Evidence**
- Historical mass extinction patterns: Five major extinction events in Earth's history, with natural background rates of 0.1 extinctions per million species per year. This provides solid baseline data.
- Human-driven acceleration: Current extinction rates are "unprecedented in human history and highly unusual in Earth's history" with documented acceleration since 1500 AD.

**MODERATE Evidence**
- Expert survey data: 2008 Future of Humanity Institute survey estimated 5% probability of AI extinction by 2100; 2016 AI expert surveys showing concern about extinction risks.
- Quantified background rates: Nature study providing upper bounds for human extinction background rates (6.9 × 10⁻⁵).

**WEAK Evidence**
- Individual AI researcher predictions of 99.9% extinction risk - appears to be outlier opinion without peer review.
- General biodiversity crisis data - relevant but doesn't directly address human extinction specifically.
- Climate change concerns - mentioned but without specific extinction probability estimates.

## Step 3: Direction of Update

**Historical Analysis:**
- Past mass extinctions (UP ~0.1%): While five major extinction events occurred, none eliminated all large mammals simultaneously. However, current human impact exceeds historical precedents.
- Human resilience patterns (DOWN ~0.2%): Humans survived the Toba supervolcanic eruption (~74,000 years ago), ice ages, plagues, and wars. Geographic distribution and technological adaptation show remarkable resilience.
- Technological risks emergence (UP ~0.3%): Unlike historical periods, we now face novel extinction risks (nuclear weapons, AI, engineered pandemics) without historical precedent for mitigation.

## Step 4: Integration

Starting from 1.0%:
- Adjustment 1: +0.1% for current extinction rate acceleration beyond historical norms
- Adjustment 2: -0.2% for demonstrated human resilience across historical catastrophes  
- Adjustment 3: +0.3% for unprecedented technological risks without historical mitigation precedents
- **Cumulative adjustment:** +0.2%

## Step 5: Sanity Checks

1. **Extremity check:** At 1.2%, I would be wrong if either: (a) technological risks prove far more dangerous than historical precedents suggest, or (b) human adaptability continues to outpace emerging threats.

2. **Base rate anchor:** The small deviation reflects that while we face novel risks, human historical resilience and adaptability remain strong factors.

3. **Update magnitude:** Conservative adjustments reflect that most dramatic predictions lack historical validation.

4. **Regression to mean:** The modest increase acknowledges uncertainty while avoiding overconfidence in extreme scenarios.

## Step 6: Final Reasoning

As a historian, I observe that humans have demonstrated extraordinary resilience across millennia of existential challenges. We survived climate shifts, supervolcanic winters, pandemics, and resource collapses that eliminated other species. However, the current period presents genuinely unprecedented risks - nuclear weapons, artificial intelligence, and engineered pandemics - that lack historical precedent for successful navigation.

The historical pattern shows that while mass extinctions occurred, they typically unfolded over geological timescales, allowing adaptation. The concern now is the potential speed of technological risks outpacing human institutional and biological adaptation rates.

Historical precedent suggests humans are remarkably adaptable, but the novelty of current risks and their potential for rapid, cascading effects represents a meaningful departure from historical patterns. The expert surveys, while showing concern, don't reach consensus on extreme probabilities.

**Evidence Summary:**
- Strong evidence pointing UP: Unprecedented extinction rate acceleration, novel technological risks
- Strong evidence pointing DOWN: Historical human resilience, survival through past catastrophes
- Net direction: SLIGHT UP

**Adjustment Calculation:**
- Base rate: 1.0%
- Total adjustment: +0.2%
- Final estimate: 1.2%

**Confidence in Adjustment:** 7

**Probability: 1.2%**

**Evidence weighted:**
- Strong: Historical mass extinction patterns: Five major extinction events in Earth's history, with natural b, Human-driven acceleration: Current extinction rates are "unprecedented in human history and highly u
- Moderate: Expert survey data: 2008 Future of Humanity Institute survey estimated 5% probability of AI extincti, Quantified background rates: Nature study providing upper bounds for human extinction background rat
- Weak: Individual AI researcher predictions of 99.9% extinction risk - appears to be outlier opinion withou, General biodiversity crisis data - relevant but doesn't directly address human extinction specifical, Climate change concerns - mentioned but without specific extinction probability estimates.

### contrarian (claude-sonnet-4-20250514)
**Prediction:** 5.0%
**Weight:** 0.8

**Reasoning:**
## Step 1: Source Analysis

The evidence is quite recent (2023-2025), with credible academic sources (Nature, Science, Stanford) mixed with media reporting. However, there's a clear bias toward sensationalism in extinction reporting - media loves apocalyptic narratives. Most concerning: the base rate may be artificially LOW because it relies on historical reference classes that don't account for humanity's unprecedented technological capabilities and global interconnectedness.

**Key bias**: The base rate assumes humans are "just another large mammal" - but we're the first species to potentially engineer our own extinction through technology.

## Step 2: Evidence Classification

**STRONG Evidence**
- Nature paper establishing background extinction rates: Provides scientific baseline but for natural extinction, not technological self-destruction
- Current biodiversity crisis data: Shows we're already in a mass extinction event, but this doesn't directly threaten human survival

**MODERATE Evidence**  
- AI extinction risk assessments (5% by 2100 from expert surveys): These are expert opinions but highly speculative about unprecedented technology
- Climate change impacts: Serious civilizational threat but unlikely to cause complete extinction given human adaptability

**WEAK Evidence**
- Single researcher claiming 99.9% AI extinction risk: Extreme outlier opinion, not peer-reviewed consensus
- Media reports on AI autonomy by 2027: Speculative timeline predictions with poor track record

## Step 3: Direction of Update

**Evidence update reasoning:**
- AI risk expert surveys: UP by approximately 3-4 percentage points because this represents new extinction pathways not captured in historical base rates
- Accelerating technological development: UP by approximately 1-2 percentage points because rate of change creates novel risks
- Current biodiversity crisis: DOWN by approximately 0.5 percentage points because it shows ecosystem resilience and human adaptability
- Historical human survival record: DOWN by approximately 1 percentage point because we've survived ice ages, plagues, wars, and natural disasters

## Step 4: Integration

Starting from 1.0%:
- Adjustment 1: +3.5% for AI and novel technological risks not in historical base rate
- Adjustment 2: +1.5% for unprecedented rate of technological/environmental change  
- Adjustment 3: -1.0% for demonstrated human resilience and adaptability
- **Cumulative adjustment:** +4.0%

## Step 5: Sanity Checks

1. **Extremity check:** At 5%, for me to be wrong, either technological risks would need to be manageable (possible - we've regulated other dangerous tech) or humans would need to be more fragile than our track record suggests.

2. **Base rate anchor:** 4x increase from base rate is justified because the base rate doesn't account for self-inflicted technological extinction - a completely new category of risk.

3. **Update magnitude:** Updates are proportional - largest adjustment for strongest novel evidence (AI risk).

4. **Regression to mean:** 5% is still quite low, accounting for uncertainty in extreme predictions.

## Step 6: Final Reasoning

As a contrarian, I'm challenging two narratives:

**Against the optimists:** The 1% base rate is dangerously low because it's based on "natural" extinction patterns. Humans are unprecedented - we're the first species that could accidentally extinct ourselves through technology. The AI risk surveys showing 5-10% aren't fringe opinions; they're from the experts building these systems. We're also living through the fastest rate of change in human history across multiple domains simultaneously.

**Against the doomsayers:** However, the extreme predictions (99.9% extinction) are likely wrong. Humans have survived existential threats before through adaptation and cooperation. We've already shown we can regulate dangerous technologies (nuclear weapons, bioweapons treaties). The current biodiversity crisis, while serious, actually demonstrates ecosystem and human resilience rather than fragility.

**The key insight everyone's missing:** The base rate methodology itself is flawed for this question. Historical reference classes can't capture the novel risk profile of a technologically advanced, globally connected civilization. But neither can speculative forecasting about unprecedented technologies.

**Evidence Summary:**
- Strong evidence pointing UP: Expert AI risk assessments, unprecedented technological development pace
- Strong evidence pointing DOWN: Historical human survival record, demonstrated technological governance capability  
- Net direction: UP (novel risks outweigh historical resilience)

**Adjustment Calculation:**
- Base rate: 1.0%
- Total adjustment: +4.0%
- Final estimate: 5.0%

**Confidence in Adjustment:** 6/10

**Probability: 5.0%**

**Evidence weighted:**
- Strong: Nature paper establishing background extinction rates: Provides scientific baseline but for natural , Current biodiversity crisis data: Shows we're already in a mass extinction event, but this doesn't d
- Moderate: AI extinction risk assessments (5% by 2100 from expert surveys): These are expert opinions but highl, Climate change impacts: Serious civilizational threat but unlikely to cause complete extinction give
- Weak: Single researcher claiming 99.9% AI extinction risk: Extreme outlier opinion, not peer-reviewed cons, Media reports on AI autonomy by 2027: Speculative timeline predictions with poor track record

### synthesizer (claude-sonnet-4-20250514)
**Prediction:** 5.3%
**Weight:** 1.2

**Reasoning:**
## Step 1: Source Analysis

The evidence spans multiple recent sources (2019-2025) with varying credibility levels. Key observations:

**Recency**: Most relevant information is very recent (2023-2025), particularly AI extinction risk assessments and updated extinction rate data.

**Credibility**: Mix of high-quality sources (Nature, Science journals, Stanford research) and more speculative content (Daily Express sensationalism). Academic sources are generally credible.

**Potential Biases**: 
- Media sources tend toward sensationalism on AI risks
- Academic sources may underestimate novel risks due to conservative methodology
- Current events may create recency bias in risk assessment

**Base Rate Consideration**: The 1.0% base rate appears to focus on historical extinction patterns for large mammals/civilizations but may not fully incorporate modern technological risks that have emerged in the last decade.

## Step 2: Evidence Classification

**STRONG Evidence**
- **AI development acceleration**: Multiple independent sources (BBC, RAND, academic surveys) indicate AI researchers themselves estimate significant extinction risks, with some predicting AI autonomy by 2027
- **Expert survey consensus**: 2008 Future of Humanity Institute survey showing 5% extinction probability by 2100 from superintelligence, supported by more recent researcher surveys

**MODERATE Evidence**
- **Accelerated environmental degradation**: UNEP data showing 60% increase in resource extraction by 2050, combined with accelerated modern extinction rates documented in Science journal
- **Sixth mass extinction context**: While not directly about human extinction, documented acceleration of species loss rates provides context for ecosystem collapse risks

**WEAK Evidence**
- **Sensationalized AI predictions**: Daily Express citing "99.9% extinction risk" - lacks methodological rigor
- **General biodiversity crisis**: While concerning, current biodiversity loss doesn't directly translate to near-term human extinction risk

## Step 3: Direction of Update

**Evidence update reasoning:**
- **AI expert consensus**: UP by approximately 2-3 percentage points because multiple independent expert surveys consistently identify AI as a novel, high-impact extinction risk not captured in historical base rates
- **Accelerated technological development timeline**: UP by approximately 1-2 percentage points because the pace of AI development has accelerated beyond historical precedent, compressing risk timelines
- **Environmental acceleration**: UP by approximately 0.5-1 percentage points because resource extraction and ecosystem degradation rates exceed historical patterns used in base rate calculation
- **Expert methodological conservatism**: DOWN by approximately 0.5 percentage points because academic extinction risk assessments may already incorporate some speculative risks

## Step 4: Integration

Starting from 1.0%:
- Adjustment 1: +2.5% for AI expert consensus on novel extinction risks
- Adjustment 2: +1.5% for accelerated technological development timelines  
- Adjustment 3: +0.75% for environmental degradation acceleration
- Adjustment 4: -0.5% for potential double-counting of speculative risks
- **Cumulative adjustment:** +4.25%

## Step 5: Sanity Checks

1. **Extremity check**: At 5.25%, this remains a low-probability event. For me to be wrong on the low side, we'd need multiple catastrophic risks to converge simultaneously or AI development to pose even greater risks than expert consensus suggests.

2. **Base rate anchor**: The ~5x increase from base rate is justified by novel technological risks (particularly AI) that weren't present in historical reference classes.

3. **Update magnitude**: Updates are proportional - largest adjustment for strongest evidence (expert AI consensus), smaller adjustments for supporting evidence.

4. **Regression to mean**: The adjustment is substantial but not extreme, maintaining skepticism while acknowledging genuinely novel risk factors.

## Step 6: Final Reasoning

As a synthesizer, I must integrate multiple perspectives while identifying what matters most. The key insight is that we're dealing with a fundamentally different risk landscape than historical reference classes captured in the base rate.

The convergence of expert opinion around AI extinction risks represents genuinely new information. Unlike historical extinction scenarios, AI development could create risks that unfold rapidly and without clear warning signals. Multiple independent sources of AI researchers - the people building these systems - express significant concern about extinction-level risks by 2100.

However, I must balance this against the strong historical precedent that humans have survived numerous existential challenges. The base rate reflects real patterns of resilience and adaptation.

The environmental evidence provides supporting context but isn't independently sufficient for large updates. Climate change and biodiversity loss create stress but rarely cause direct extinction of adaptable species like humans.

My synthesis weighs novel technological risks heavily while maintaining appropriate skepticism about extreme predictions.

**Evidence Summary:**
- Strong evidence pointing UP: AI expert consensus on extinction risks, accelerated AI development timelines
- Strong evidence pointing DOWN: Historical human resilience patterns (already in base rate)
- Net direction: UP

**Adjustment Calculation:**
- Base rate: 1.0%
- Total adjustment: +4.25%
- Final estimate: 5.25%

**Confidence in Adjustment:** 7/10

**Probability: 5.3%**

**Evidence weighted:**
- Strong: **AI development acceleration**: Multiple independent sources (BBC, RAND, academic surveys) indicate, **Expert survey consensus**: 2008 Future of Humanity Institute survey showing 5% extinction probabil
- Moderate: **Accelerated environmental degradation**: UNEP data showing 60% increase in resource extraction by , **Sixth mass extinction context**: While not directly about human extinction, documented acceleratio
- Weak: **Sensationalized AI predictions**: Daily Express citing "99.9% extinction risk" - lacks methodologi, **General biodiversity crisis**: While concerning, current biodiversity loss doesn't directly transl

## Ensemble Aggregation

| Agent | Model | Weight | Prediction |
|-------|-------|--------|------------|
| analyst | claude-sonnet-4-20250514 | 1.0 | 4.3% |
| historian | claude-sonnet-4-20250514 | 1.0 | 1.2% |
| contrarian | claude-sonnet-4-20250514 | 0.8 | 5.0% |
| synthesizer | claude-sonnet-4-20250514 | 1.2 | 5.3% |

**Aggregation method:** weighted_average
**Weighted average:** 3.96% → **Submitted: 4.0%**

## Calibration Checklist

- [x] **paraphrase:** Will humans go extinct before 2100?
- [x] **base_rate_grounded:** Base rate: 1.0%, Final: 4.0%
- [x] **consistency_test:** Automated check passed
- [x] **evidence_audit:** Evidence recorded in artifacts
- [x] **blind_spots:** To be reviewed manually
- [x] **status_quo_bias:** To be reviewed manually

## Costs
- Research API calls: $0.01
- LLM calls: $0.11
- **Total: $0.12**

---
*Generated at 2026-01-24T09:11:54.292841Z*