"""
Search Pipeline

Key differences from previous approach:
1. Forecasters generate search queries directly in their responses
2. Queries are tagged with source (Google, Google News, Agent)
3. AskNews is called programmatically for current search (not LLM-controlled)
4. Agentic search uses GPT to iteratively research
5. Articles are summarized with question context
"""

import asyncio
import logging
import os
import re
import traceback
from dataclasses import dataclass
from typing import Any

import dateparser
import httpx

from ..utils.llm import LLMClient, get_cost_tracker
from .content_extractor import FastContentExtractor
from .prompts import CONTINUATION_SEARCH_PROMPT, INITIAL_SEARCH_PROMPT

logger = logging.getLogger(__name__)


@dataclass
class QuestionDetails:
    """Question details for context-aware article summarization."""

    title: str
    resolution_criteria: str
    fine_print: str
    description: str
    resolution_date: str | None = None


@dataclass
class SearchResult:
    """A search result with content."""

    url: str
    title: str
    content: str
    source: str  # google, google_news, asknews, agent
    date: str | None = None


# Article summarization prompt template
ARTICLE_SUMMARY_PROMPT = """
You are an assistant to a superforecaster and your task involves high-quality information retrieval to help the forecaster make the most informed forecasts. Forecasting involves parsing through an immense trove of internet articles and web content. To make this easier for the forecaster, you read entire articles and extract the key pieces of the articles relevant to the question. The key pieces generally include:

1. Facts, statistics and other objective measurements described in the article
2. Opinions from reliable and named sources (e.g. if the article writes 'according to a 2023 poll by Gallup' or 'The 2025 presidential approval rating poll by Reuters' etc.)
3. Potentially useful opinions from less reliable/not-named sources (you explicitly document the less reliable origins of these opinions though)

Today, you're focusing on the question:

{title}

Resolution criteria:
{resolution_criteria}

Fine print:
{fine_print}

Background information:
{background}

Article to summarize:
{article}

Note: If the web content extraction is incomplete or you believe the quality of the extracted content isn't the best, feel free to add a disclaimer before your summary.

Please summarize only the article given, not injecting your own knowledge or providing a forecast. Aim to achieve a balance between a superficial summary and an overly verbose account.
"""


class SearchPipeline:
    """
    Search pipeline that processes queries generated by forecasters.

    Supports multiple search sources:
    - Google (web search via Serper)
    - Google News (news search via Serper)
    - AskNews (news + deep research, called programmatically for current search)
    - Agent (agentic search with iterative GPT analysis)
    """

    def __init__(self, config: dict, llm_client: LLMClient | None = None):
        """
        Initialize search pipeline.

        Args:
            config: Configuration dict with model settings
            llm_client: Optional LLMClient instance (creates one if not provided)
        """
        self.config = config
        if llm_client:
            self.llm = llm_client
        else:
            llm_timeout = config.get("llm", {}).get("timeout_seconds")
            self.llm = LLMClient(timeout_seconds=llm_timeout)
        self.http_client: httpx.AsyncClient | None = None

        # API keys
        self.serper_key = os.getenv("SERPER_API_KEY") or os.getenv("SERPER_KEY")
        self.asknews_client_id = os.getenv("ASKNEWS_CLIENT_ID")
        self.asknews_secret = os.getenv("ASKNEWS_CLIENT_SECRET") or os.getenv("ASKNEWS_SECRET")

        # Semaphore to limit concurrent AskNews calls (free tier has concurrency limit)
        self._asknews_semaphore = asyncio.Semaphore(1)

    async def __aenter__(self):
        self.http_client = httpx.AsyncClient(timeout=70.0)
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.http_client:
            await self.http_client.aclose()
            self.http_client = None

    # -------------------------------------------------------------------------
    # Main entry point: process search queries from forecaster response
    # -------------------------------------------------------------------------

    async def execute_searches_from_response(
        self,
        response: str,
        search_id: str,
        question_details: QuestionDetails,
        include_asknews: bool = False,
    ) -> tuple[str, dict[str, Any]]:
        """
        Parse search queries from a response and execute them.

        Queries are expected in format:
        Search queries:
        1. "query text" (Google)
        2. "query text" (Google News)
        3. "query text" (Agent)

        Args:
            response: The response containing search queries (from query generation)
            search_id: Identifier for logging (e.g., "historical", "current")
            question_details: Question context for summarization
            include_asknews: If True, also call AskNews with question title (for current search)

        Returns:
            Tuple of (formatted_results_string, metadata_dict)
        """
        cost_tracker = get_cost_tracker()
        start_cost = cost_tracker.total_cost

        metadata = {
            "search_id": search_id,
            "searched": False,
            "num_queries": 0,
            "queries": [],
            "tools_used": set(),
        }

        # Track costs by operation type
        self._current_summarization_cost = 0.0
        self._current_agentic_cost = 0.0
        try:
            # Extract the "Search queries:" block
            search_queries_block = re.search(
                r"(?:Search queries:)(.*)", response, re.DOTALL | re.IGNORECASE
            )
            if not search_queries_block:
                logger.info(f"Search[{search_id}]: No search queries block found")
                return "", metadata

            queries_text = search_queries_block.group(1).strip()

            # Parse queries with sources
            # Format: 1. "text" (Source) or 1. text (Source) or 1. text [Source]
            search_queries = re.findall(
                r'(?:\d+\.\s*)?(["\']?(.*?)["\']?)\s*[\(\[](Google|Google News|Agent|AskNews)[\)\]]',
                queries_text,
            )

            # Fallback to unquoted queries
            if not search_queries:
                search_queries = re.findall(
                    r"(?:\d+\.\s*)?([^(\[\n]+)\s*[\(\[](Google|Google News|Agent|AskNews)[\)\]]",
                    queries_text,
                )

            if not search_queries:
                logger.info(f"Search[{search_id}]: No valid search queries found:\n{queries_text}")
                return "", metadata

            logger.info(f"Search[{search_id}]: Processing {len(search_queries)} search queries")

            metadata["searched"] = True
            metadata["num_queries"] = len(search_queries)

            # Build tasks for each query
            tasks = []
            query_sources = []

            # Track LLM-generated AskNews query for Deep Research
            asknews_deep_research_query: str | None = None

            for match in search_queries:
                if len(match) == 3:
                    _, raw_query, source = match
                else:
                    raw_query, source = match

                query = raw_query.strip().strip('"').strip("'")
                if not query:
                    continue

                logger.info(f"Search[{search_id}]: Query='{query}' Source={source}")

                # Track query and tool usage in metadata
                metadata["queries"].append({"query": query, "tool": source})
                metadata["tools_used"].add(source)

                if source in ("Google", "Google News"):
                    query_sources.append((query, source))
                    tasks.append(
                        self._google_search_and_scrape(
                            query=query,
                            is_news=(source == "Google News"),
                            question_details=question_details,
                            date_before=question_details.resolution_date,
                        )
                    )
                elif source == "Agent":
                    query_sources.append((query, source))
                    tasks.append(self._agentic_search(query))
                elif source == "AskNews":
                    # Store the LLM-generated query for Deep Research
                    # The actual AskNews call will be added below with include_asknews
                    asknews_deep_research_query = query
                    logger.info(
                        f"Search[{search_id}]: Found AskNews Deep Research query: {query[:50]}..."
                    )

            # Programmatically add AskNews for current search
            # Uses question title for news search, LLM-generated query for Deep Research
            if include_asknews:
                logger.info(f"Search[{search_id}]: Adding AskNews search with question title")
                if asknews_deep_research_query:
                    logger.info(f"Search[{search_id}]: Deep Research will use LLM-generated query")
                tasks.append(
                    self._call_asknews(
                        news_query=question_details.title,
                        deep_research_query=asknews_deep_research_query,
                    )
                )
                query_sources.append((question_details.title, "AskNews"))
                # Track in metadata (include both queries if Deep Research query exists)
                asknews_metadata = {"query": question_details.title, "tool": "AskNews"}
                if asknews_deep_research_query:
                    asknews_metadata["deep_research_query"] = asknews_deep_research_query
                metadata["queries"].append(asknews_metadata)
                metadata["tools_used"].add("AskNews")

            if not tasks:
                logger.info(f"Search[{search_id}]: No tasks generated")
                return "", metadata

            # Execute all tasks
            results = await asyncio.gather(*tasks, return_exceptions=True)

            # Format outputs and count results
            formatted_results = ""
            for i, ((query, source), result) in enumerate(zip(query_sources, results, strict=True)):
                if isinstance(result, Exception):
                    logger.error(f"Search[{search_id}]: Error for '{query}' â†’ {result}")
                    metadata["queries"][i]["success"] = False
                    metadata["queries"][i]["error"] = str(result)
                    metadata["queries"][i]["num_results"] = 0

                    if source == "AskNews":
                        formatted_results += f"\n<Asknews_articles>\nQuery: {query}\nError retrieving results: {result}\n</Asknews_articles>\n"
                    elif source == "Agent":
                        formatted_results += (
                            f"\n<Agent_report>\nQuery: {query}\nError: {result}\n</Agent_report>\n"
                        )
                    else:
                        formatted_results += (
                            f'\n<Summary query="{query}">\nError: {result}\n</Summary>\n'
                        )
                else:
                    logger.info(f"Search[{search_id}]: Query '{query}' processed successfully")

                    # Count results by looking for tags in the result string
                    if source == "AskNews":
                        num_results = result.count("**") // 2  # AskNews articles have ** in titles
                        formatted_results += (
                            f"\n<Asknews_articles>\nQuery: {query}\n{result}</Asknews_articles>\n"
                        )
                    elif source == "Agent":
                        # Agent returns 1 synthesized analysis (not multiple raw contents)
                        # Count as 1 if substantial content exists, 0 if error or empty
                        num_results = 1 if len(result) > 500 and "Error:" not in result[:100] else 0
                        formatted_results += (
                            f"\n<Agent_report>\nQuery: {query}\n{result}</Agent_report>\n"
                        )
                    else:
                        num_results = result.count("<Summary")
                        formatted_results += result

                    metadata["queries"][i]["success"] = True
                    metadata["queries"][i]["num_results"] = num_results

            # Convert set to list for JSON serialization
            metadata["tools_used"] = list(metadata["tools_used"])

            # Track LLM costs by operation type
            metadata["llm_cost"] = round(cost_tracker.total_cost - start_cost, 4)
            metadata["llm_cost_summarization"] = round(self._current_summarization_cost, 4)
            metadata["llm_cost_agentic"] = round(self._current_agentic_cost, 4)

            return formatted_results, metadata

        except Exception as e:
            logger.error(f"Search[{search_id}]: Error processing search queries: {e}")
            traceback.print_exc()
            metadata["error"] = str(e)
            return (
                "Error processing some search queries. Partial results may be available.",
                metadata,
            )

    # -------------------------------------------------------------------------
    # Google Search + Scrape
    # -------------------------------------------------------------------------

    async def _google_search(
        self,
        query: str,
        is_news: bool = False,
        date_before: str | None = None,
    ) -> list[str]:
        """
        Search Google via Serper API.

        Args:
            query: Search query
            is_news: Use Google News instead of web search
            date_before: Only include results before this date

        Returns:
            List of URLs
        """
        if not self.serper_key:
            logger.warning("SERPER_API_KEY not set, skipping Google search")
            return []

        # Clean query
        query = query.replace('"', "").replace("'", "").strip()
        logger.debug(f"[google_search] Query: '{query}' is_news={is_news}")

        search_type = "news" if is_news else "search"
        url = f"https://google.serper.dev/{search_type}"

        try:
            response = await self.http_client.post(
                url,
                headers={"X-API-KEY": self.serper_key, "Content-Type": "application/json"},
                json={"q": query, "num": 20},
            )
            response.raise_for_status()
            data = response.json()

            items = data.get("news" if is_news else "organic", [])
            logger.debug(f"[google_search] Found {len(items)} raw results")

            # Filter by date if specified
            filtered_items = []
            for item in items:
                if date_before:
                    item_date_str = item.get("date", "")
                    item_date = self._parse_date(item_date_str)
                    if item_date != "Unknown" and self._validate_time(date_before, item_date):
                        filtered_items.append(item)
                    # Skip items we can't validate
                else:
                    filtered_items.append(item)

                if len(filtered_items) >= 12:
                    break

            urls = [item["link"] for item in filtered_items]
            logger.info(f"[google_search] Returning {len(urls)} URLs")
            return urls

        except Exception as e:
            logger.error(f"[google_search] Error: {e}")
            return []

    async def _google_search_and_scrape(
        self,
        query: str,
        is_news: bool,
        question_details: QuestionDetails,
        date_before: str | None = None,
    ) -> str:
        """
        Search Google and scrape/summarize results.

        Args:
            query: Search query
            is_news: Use Google News
            question_details: Question context for summarization
            date_before: Date filter

        Returns:
            Formatted summary string
        """
        logger.debug(f"[google_search_and_scrape] query='{query}' is_news={is_news}")

        try:
            urls = await self._google_search(query, is_news, date_before)

            if not urls:
                logger.warning(f"[google_search_and_scrape] No URLs returned for: '{query}'")
                return f'<Summary query="{query}">No URLs returned from Google.</Summary>\n'

            # Extract content from URLs
            async with FastContentExtractor() as extractor:
                logger.info(f"[google_search_and_scrape] Extracting content from {len(urls)} URLs")
                results = await extractor.extract_content(urls)

            # Summarize top results
            summarize_tasks = []
            valid_urls = []
            max_results = 3

            for url, data in results.items():
                if len(summarize_tasks) >= max_results:
                    break

                content = (data.get("content") or "").strip()
                if len(content.split()) < 100:
                    logger.debug(f"[google_search_and_scrape] Skipping low-content: {url}")
                    continue

                if content:
                    truncated = content[:8000]
                    logger.debug(
                        f"[google_search_and_scrape] Summarizing {len(truncated)} chars from {url}"
                    )
                    summarize_tasks.append(self._summarize_article(truncated, question_details))
                    valid_urls.append(url)

            if not summarize_tasks:
                logger.warning("[google_search_and_scrape] No content to summarize")
                return f'<Summary query="{query}">No usable content extracted from any URL.</Summary>\n'

            summaries = await asyncio.gather(*summarize_tasks, return_exceptions=True)

            # Format output
            output = ""
            for url, summary in zip(valid_urls, summaries, strict=True):
                if isinstance(summary, Exception):
                    logger.error(f"[google_search_and_scrape] Error summarizing {url}: {summary}")
                    output += f'\n<Summary source="{url}">\nError: {summary}\n</Summary>\n'
                else:
                    output += f'\n<Summary source="{url}">\n{summary}\n</Summary>\n'

            return output

        except Exception as e:
            logger.error(f"[google_search_and_scrape] Error: {e}")
            traceback.print_exc()
            return f'<Summary query="{query}">Error during search and scrape: {e}</Summary>\n'

    async def _summarize_article(
        self,
        article: str,
        question_details: QuestionDetails,
    ) -> str:
        """Summarize an article with question context."""
        prompt = ARTICLE_SUMMARY_PROMPT.format(
            title=question_details.title,
            resolution_criteria=question_details.resolution_criteria,
            fine_print=question_details.fine_print,
            background=question_details.description,
            article=article,
        )

        # Use haiku for summarization (fast, cheap)
        active_models = self.config.get("active_models", {})
        model = active_models.get(
            "article_summarizer",
            self.config.get("models", {}).get("article_summarizer", "claude-3-haiku-20240307"),
        )

        try:
            response = await self.llm.complete(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=1500,
            )
            # Track summarization cost
            self._current_summarization_cost += response.cost
            return response.content
        except Exception as e:
            logger.error(f"Article summarization failed: {e}")
            return f"Error summarizing article: {e}"

    # -------------------------------------------------------------------------
    # AskNews
    # -------------------------------------------------------------------------

    async def _call_asknews(
        self,
        news_query: str,
        deep_research_query: str | None = None,
    ) -> str:
        """
        Search AskNews for relevant articles and run Deep Research.

        Uses three strategies:
        1. Latest news (recent/hot articles) - uses news_query
        2. Historical news (news knowledge archive) - uses news_query
        3. Deep Research (AI-synthesized analysis) - uses deep_research_query if provided

        Args:
            news_query: Query for news search (typically question title)
            deep_research_query: Query for Deep Research (LLM-generated, should request
                information rather than ask a yes/no forecasting question)

        Uses semaphore to respect AskNews free tier concurrency limits.
        """
        if not self.asknews_client_id or not self.asknews_secret:
            logger.warning("AskNews credentials not set")
            return "AskNews credentials not configured."

        # Use semaphore to limit concurrent AskNews calls (free tier limit)
        async with self._asknews_semaphore:
            formatted_articles = ""

            # Part 1: News search (latest + historical)
            try:
                from asknews_sdk import AskNewsSDK

                ask = AskNewsSDK(
                    client_id=self.asknews_client_id,
                    client_secret=self.asknews_secret,
                    scopes=set(["news"]),
                )

                # Run searches sequentially with rate limit delay
                # AskNews Pro tier (via Metaculus): 1 request per 10 seconds, add 2s buffer
                rate_limit_delay = 12

                logger.debug(f"[call_asknews] Searching latest news for: {news_query[:50]}...")
                hot_response = await asyncio.to_thread(
                    ask.news.search_news,
                    query=news_query,
                    n_articles=8,
                    return_type="both",
                    strategy="latest news",
                )

                # Rate limit delay between calls
                logger.debug(f"[call_asknews] Waiting {rate_limit_delay}s for rate limit...")
                await asyncio.sleep(rate_limit_delay)

                logger.debug(f"[call_asknews] Searching historical news for: {news_query[:50]}...")
                historical_response = await asyncio.to_thread(
                    ask.news.search_news,
                    query=news_query,
                    n_articles=8,
                    return_type="both",
                    strategy="news knowledge",
                )

                # Combine and deduplicate articles by URL
                hot_articles = hot_response.as_dicts or []
                historical_articles = historical_response.as_dicts or []

                seen_urls = set()
                unique_articles = []

                # Process hot articles first (they're more recent)
                for article in hot_articles:
                    article_dict = article.__dict__
                    url = article_dict.get("article_url", "")
                    if url and url not in seen_urls:
                        seen_urls.add(url)
                        unique_articles.append(article_dict)

                # Then add historical articles that aren't duplicates
                for article in historical_articles:
                    article_dict = article.__dict__
                    url = article_dict.get("article_url", "")
                    if url and url not in seen_urls:
                        seen_urls.add(url)
                        unique_articles.append(article_dict)

                # Sort by date (newest first)
                unique_articles = sorted(unique_articles, key=lambda x: x["pub_date"], reverse=True)

                duplicates_removed = (len(hot_articles) + len(historical_articles)) - len(
                    unique_articles
                )
                if duplicates_removed > 0:
                    logger.info(f"[call_asknews] Removed {duplicates_removed} duplicate articles")

                # Format news results
                formatted_articles = "Here are the relevant news articles:\n\n"

                for article in unique_articles:
                    pub_date = article["pub_date"].strftime("%B %d, %Y %I:%M %p")
                    formatted_articles += (
                        f"**{article['eng_title']}**\n"
                        f"{article['summary']}\n"
                        f"Original language: {article['language']}\n"
                        f"Publish date: {pub_date}\n"
                        f"Source:[{article['source_id']}]({article['article_url']})\n\n"
                    )

                if not unique_articles:
                    formatted_articles += "No articles were found.\n\n"

            except Exception as e:
                logger.error(f"[call_asknews] News search error: {e}")
                formatted_articles = f"Error retrieving news articles: {e}\n\n"

            # Part 2: Deep Research (only if deep_research_query provided)
            if deep_research_query:
                try:
                    logger.debug(
                        f"[call_asknews] Waiting {rate_limit_delay}s before deep research..."
                    )
                    await asyncio.sleep(rate_limit_delay)

                    logger.info(
                        f"[call_asknews] Running deep research for: {deep_research_query[:50]}..."
                    )
                    deep_research_result = await self._call_asknews_deep_research(
                        deep_research_query, preset="low-depth", _skip_semaphore=True
                    )

                    formatted_articles += (
                        f"\n--- Deep Research Analysis ---\n{deep_research_result}\n"
                    )
                    logger.info(
                        f"[call_asknews] Deep research complete, got {len(deep_research_result)} chars"
                    )

                except Exception as e:
                    logger.error(f"[call_asknews] Deep research error: {e}")
                    formatted_articles += f"\n--- Deep Research Analysis ---\nError: {e}\n"
            else:
                logger.info(
                    "[call_asknews] Skipping deep research (no deep_research_query provided)"
                )

            return formatted_articles

    async def _call_asknews_deep_research(
        self,
        query: str,
        preset: str = "low-depth",
        _skip_semaphore: bool = False,
    ) -> str:
        """
        Use AskNews Deep Research for AI-synthesized research.

        Matches the template bot's implementation with three depth presets.

        Args:
            query: Research query/question
            preset: One of "low-depth", "medium-depth", "high-depth"
            _skip_semaphore: Internal flag to skip semaphore when called from _call_asknews

        Returns:
            Formatted research text
        """
        if not self.asknews_client_id or not self.asknews_secret:
            logger.warning("AskNews credentials not set")
            return "AskNews credentials not configured."

        async def _do_deep_research():
            try:
                from asknews_sdk import AsyncAskNewsSDK
                from asknews_sdk.dto.deepnews import CreateDeepNewsResponse
            except ImportError:
                logger.error(
                    "asknews_sdk not installed or outdated. Run: poetry add asknews@0.11.6"
                )
                return "AskNews SDK not available for deep research."

            # Configure based on preset
            # Metaculus plan only allows asknews as source
            if preset == "low-depth":
                sources = ["asknews"]
                search_depth = 1
                max_depth = 1
                filter_params = None
            elif preset == "medium-depth":
                sources = ["asknews"]
                search_depth = 2
                max_depth = 4
                filter_params = None
            elif preset == "high-depth":
                sources = ["asknews"]
                search_depth = 4
                max_depth = 6
                filter_params = None
            else:
                logger.warning(f"Unknown preset '{preset}', using low-depth")
                sources = ["asknews"]
                search_depth = 1
                max_depth = 1
                filter_params = None

            model = "deepseek-basic"  # Default model matching template

            logger.info(
                f"[asknews_deep_research] Starting {preset} research: {query[:50]}... "
                f"(sources={sources}, depth={search_depth}/{max_depth})"
            )

            try:
                async with AsyncAskNewsSDK(
                    client_id=self.asknews_client_id,
                    client_secret=self.asknews_secret,
                    scopes={"chat", "news", "stories", "analytics"},
                ) as sdk:
                    response = await sdk.chat.get_deep_news(
                        messages=[{"role": "user", "content": query}],
                        search_depth=search_depth,
                        max_depth=max_depth,
                        sources=sources,
                        stream=False,
                        return_sources=False,
                        model=model,
                        inline_citations="numbered",
                        filter_params=filter_params,
                    )

                    if not isinstance(response, CreateDeepNewsResponse):
                        raise ValueError("Response is not a CreateDeepNewsResponse")

                    text = response.choices[0].message.content

                    # Extract content from <final_answer> tags if present
                    start_tag = "<final_answer>"
                    end_tag = "</final_answer>"
                    start_index = text.find(start_tag)

                    if start_index != -1:
                        start_index += len(start_tag)
                        end_index = text.find(end_tag, start_index)
                        if end_index != -1:
                            text = text[start_index:end_index].strip()

                    logger.info(f"[asknews_deep_research] Complete, got {len(text)} chars")
                    return text

            except Exception as e:
                logger.error(f"[asknews_deep_research] Error: {e}")
                return f"Error running deep research: {e}"

        # If called from _call_asknews, semaphore is already held
        if _skip_semaphore:
            return await _do_deep_research()
        else:
            async with self._asknews_semaphore:
                return await _do_deep_research()

    # -------------------------------------------------------------------------
    # Agentic Search
    # -------------------------------------------------------------------------

    async def _agentic_search(self, query: str) -> str:
        """
        Perform iterative agentic search using LLM.

        The LLM generates search queries, analyzes results, and iterates
        until it has enough information or reaches max steps.
        """
        logger.info(f"[agentic_search] Starting research for: {query}")

        max_steps = self.config.get("research", {}).get("agentic_search_max_steps", 7)
        current_analysis = ""
        all_search_queries: list[str] = []
        search_results = ""

        # Get model for agentic search from active models (respects mode)
        active_models = self.config.get("active_models", {})
        model = active_models.get(
            "agentic_search",
            # Fallback: check if there's a query_generator, else default to haiku
            active_models.get("query_generator", "openrouter/anthropic/claude-3.5-haiku"),
        )

        for step in range(max_steps):
            try:
                # Build prompt
                if step == 0:
                    prompt = INITIAL_SEARCH_PROMPT.format(query=query)
                else:
                    if current_analysis:
                        previous_section = (
                            f"Your previous analysis:\n{current_analysis}\n\n"
                            f"Previous search queries used: {', '.join(all_search_queries)}\n"
                        )
                    else:
                        previous_section = (
                            f"Previous search queries used: {', '.join(all_search_queries)}\n"
                        )

                    prompt = CONTINUATION_SEARCH_PROMPT.format(
                        query=query,
                        previous_section=previous_section,
                        search_results=search_results,
                    )

                logger.info(f"[agentic_search] Step {step + 1}: Calling LLM")
                response = await self.llm.complete(
                    model=model,
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=4000,
                )

                # Track agentic search cost
                self._current_agentic_cost += response.cost

                response_text = response.content

                # Parse analysis
                analysis_match = re.search(
                    r"Analysis:\s*(.*?)(?=Search queries:|$)", response_text, re.DOTALL
                )
                if not analysis_match:
                    logger.warning("[agentic_search] Could not parse analysis from response")
                    return f"Error: Failed to parse analysis at step {step + 1}"

                if step > 0:
                    current_analysis = analysis_match.group(1).strip()
                    logger.info(
                        f"[agentic_search] Step {step + 1}: Analysis updated ({len(current_analysis)} chars)"
                    )

                # Check for search queries
                search_queries_match = re.search(
                    r"Search queries:\s*(.*)", response_text, re.DOTALL
                )

                if step == 0 and not search_queries_match:
                    logger.warning("[agentic_search] No search queries in initial response")
                    return "Error: Failed to generate initial search queries"

                if not search_queries_match or step == max_steps - 1:
                    if step > 0:
                        logger.info(f"[agentic_search] Research complete at step {step + 1}")
                        break

                # Extract queries with sources
                queries_text = search_queries_match.group(1).strip()
                search_queries_with_source = re.findall(
                    r"\d+\.\s*([^(]+?)\s*\((Google|Google News)\)", queries_text
                )

                if not search_queries_with_source:
                    if step == 0:
                        logger.warning("[agentic_search] No valid queries in initial response")
                        return "Error: Failed to parse initial search queries"
                    else:
                        logger.info("[agentic_search] No new queries, completing research")
                        break

                # Limit to 5 queries
                search_queries_with_source = [
                    (q.strip(), source) for q, source in search_queries_with_source[:5]
                ]

                logger.info(
                    f"[agentic_search] Step {step + 1}: Found {len(search_queries_with_source)} queries"
                )
                all_search_queries.extend([q for q, _ in search_queries_with_source])

                # Execute searches
                search_tasks = []
                for sq, source in search_queries_with_source:
                    logger.debug(f"[agentic_search] Searching: {sq} (Source: {source})")
                    search_tasks.append(
                        self._google_search_agentic(sq, is_news=(source == "Google News"))
                    )

                search_results_list = await asyncio.gather(*search_tasks, return_exceptions=True)

                # Format search results
                search_results = ""
                for (sq, source), result in zip(
                    search_queries_with_source, search_results_list, strict=True
                ):
                    if isinstance(result, Exception):
                        search_results += (
                            f"\nSearch query: {sq} (Source: {source})\nError: {result}\n"
                        )
                    else:
                        search_results += f"\nSearch query: {sq} (Source: {source})\n{result}\n"

                logger.info(
                    f"[agentic_search] Step {step + 1}: Search complete, {len(search_results)} chars"
                )

            except Exception as e:
                logger.error(f"[agentic_search] Error at step {step + 1}: {e}")
                if current_analysis:
                    break
                else:
                    return f"Error during agentic search: {e}"

        if not current_analysis:
            return "Error: No analysis was generated during the research process"

        logger.info(f"[agentic_search] Complete: {step + 1} steps")
        return current_analysis

    async def _google_search_agentic(self, query: str, is_news: bool = False) -> str:
        """
        Google search that returns raw content (no summarization).

        Used by agentic search where the agent analyzes raw content.
        """
        logger.debug(f"[google_search_agentic] query='{query}' is_news={is_news}")

        try:
            urls = await self._google_search(query, is_news)

            if not urls:
                return f'<RawContent query="{query}">No URLs returned from Google.</RawContent>\n'

            async with FastContentExtractor() as extractor:
                results = await extractor.extract_content(urls)

            output = ""
            max_results = 3
            results_count = 0

            for url, data in results.items():
                if results_count >= max_results:
                    break

                content = (data.get("content") or "").strip()
                if len(content.split()) < 100:
                    continue

                if content:
                    truncated = content[:8000]
                    output += f'\n<RawContent source="{url}">\n{truncated}\n</RawContent>\n'
                    results_count += 1

            if not output:
                return f'<RawContent query="{query}">No usable content extracted.</RawContent>\n'

            return output

        except Exception as e:
            logger.error(f"[google_search_agentic] Error: {e}")
            return f'<RawContent query="{query}">Error during search: {e}</RawContent>\n'

    # -------------------------------------------------------------------------
    # Utility methods
    # -------------------------------------------------------------------------

    def _parse_date(self, date_str: str) -> str:
        """Parse date string to standardized format."""
        if not date_str:
            return "Unknown"
        parsed_date = dateparser.parse(date_str, settings={"STRICT_PARSING": False})
        if parsed_date:
            return parsed_date.strftime("%b %d, %Y")
        return "Unknown"

    def _validate_time(self, before_date_str: str, source_date_str: str) -> bool:
        """Check if source date is before the cutoff date."""
        if source_date_str == "Unknown":
            return False
        before_date = dateparser.parse(before_date_str)
        source_date = dateparser.parse(source_date_str)
        if before_date and source_date:
            return source_date <= before_date
        return False


# -------------------------------------------------------------------------
# Convenience functions
# -------------------------------------------------------------------------


async def execute_searches_from_response(
    response: str,
    search_id: str,
    question_details: dict,
    config: dict,
    llm_client: LLMClient | None = None,
) -> tuple[str, dict[str, Any]]:
    """
    Convenience function to process search queries from a response.

    Args:
        response: Response containing search queries (from query generation)
        search_id: Identifier for logging (e.g., "historical", "current")
        question_details: Dict with title, resolution_criteria, fine_print, description
        config: Configuration dict
        llm_client: Optional LLMClient instance

    Returns:
        Tuple of (formatted_search_results_string, metadata_dict)
    """
    qd = QuestionDetails(
        title=question_details.get("title", ""),
        resolution_criteria=question_details.get("resolution_criteria", ""),
        fine_print=question_details.get("fine_print", ""),
        description=question_details.get("description", ""),
        resolution_date=question_details.get("resolution_date"),
    )

    async with SearchPipeline(config, llm_client) as pipeline:
        return await pipeline.execute_searches_from_response(response, search_id, qd)
