"""
Search Pipeline - Port from Panshul42's tournament-winning implementation

Key differences from previous approach:
1. Forecasters generate search queries directly in their responses
2. Queries are tagged with source (Google, Google News, Assistant, Agent)
3. Agentic search uses GPT to iteratively research
4. Articles are summarized with question context
"""

import asyncio
import re
import os
import logging
import traceback
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
from datetime import datetime, timezone

import httpx
import dateparser

from ..utils.llm import LLMClient
from .content_extractor import FastContentExtractor
from .prompts_panshul42 import INITIAL_SEARCH_PROMPT, CONTINUATION_SEARCH_PROMPT

logger = logging.getLogger(__name__)


@dataclass
class QuestionDetails:
    """Question details for context-aware article summarization."""
    title: str
    resolution_criteria: str
    fine_print: str
    description: str
    resolution_date: Optional[str] = None


@dataclass
class SearchResult:
    """A search result with content."""
    url: str
    title: str
    content: str
    source: str  # google, google_news, asknews, agent
    date: Optional[str] = None


# Article summarization prompt template
ARTICLE_SUMMARY_PROMPT = """
You are an assistant to a superforecaster and your task involves high-quality information retrieval to help the forecaster make the most informed forecasts. Forecasting involves parsing through an immense trove of internet articles and web content. To make this easier for the forecaster, you read entire articles and extract the key pieces of the articles relevant to the question. The key pieces generally include:

1. Facts, statistics and other objective measurements described in the article
2. Opinions from reliable and named sources (e.g. if the article writes 'according to a 2023 poll by Gallup' or 'The 2025 presidential approval rating poll by Reuters' etc.)
3. Potentially useful opinions from less reliable/not-named sources (you explicitly document the less reliable origins of these opinions though)

Today, you're focusing on the question:

{title}

Resolution criteria:
{resolution_criteria}

Fine print:
{fine_print}

Background information:
{background}

Article to summarize:
{article}

Note: If the web content extraction is incomplete or you believe the quality of the extracted content isn't the best, feel free to add a disclaimer before your summary.

Please summarize only the article given, not injecting your own knowledge or providing a forecast. Aim to achieve a balance between a superficial summary and an overly verbose account.
"""


class SearchPipeline:
    """
    Search pipeline that processes queries generated by forecasters.

    Supports multiple search sources:
    - Google (web search via Serper)
    - Google News (news search via Serper)
    - Assistant (AskNews)
    - Agent (agentic search with iterative GPT analysis)
    """

    def __init__(self, config: dict, llm_client: Optional[LLMClient] = None):
        """
        Initialize search pipeline.

        Args:
            config: Configuration dict with model settings
            llm_client: Optional LLMClient instance (creates one if not provided)
        """
        self.config = config
        self.llm = llm_client or LLMClient()
        self.http_client: Optional[httpx.AsyncClient] = None

        # API keys
        self.serper_key = os.getenv("SERPER_API_KEY") or os.getenv("SERPER_KEY")
        self.asknews_client_id = os.getenv("ASKNEWS_CLIENT_ID")
        self.asknews_secret = os.getenv("ASKNEWS_CLIENT_SECRET") or os.getenv("ASKNEWS_SECRET")

        # Semaphore to limit concurrent AskNews calls (free tier has concurrency limit)
        self._asknews_semaphore = asyncio.Semaphore(1)

    async def __aenter__(self):
        self.http_client = httpx.AsyncClient(timeout=70.0)
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.http_client:
            await self.http_client.aclose()
            self.http_client = None

    # -------------------------------------------------------------------------
    # Main entry point: process search queries from forecaster response
    # -------------------------------------------------------------------------

    async def process_search_queries(
        self,
        response: str,
        forecaster_id: str,
        question_details: QuestionDetails,
    ) -> str:
        """
        Parse search queries from forecaster's response and execute them.

        Queries are expected in format:
        Search queries:
        1. "query text" (Google)
        2. "query text" (Google News)
        3. "query text" (Assistant)
        4. "query text" (Agent)

        Args:
            response: The forecaster's response containing search queries
            forecaster_id: ID for logging
            question_details: Question context for summarization

        Returns:
            Formatted string with search results
        """
        try:
            # Extract the "Search queries:" block
            search_queries_block = re.search(
                r'(?:Search queries:)(.*)',
                response,
                re.DOTALL | re.IGNORECASE
            )
            if not search_queries_block:
                logger.info(f"Forecaster {forecaster_id}: No search queries block found")
                return ""

            queries_text = search_queries_block.group(1).strip()

            # Parse queries with sources
            # Format: 1. "text" (Source) or 1. text (Source)
            search_queries = re.findall(
                r'(?:\d+\.\s*)?(["\']?(.*?)["\']?)\s*\((Google|Google News|Assistant|Agent|Perplexity)\)',
                queries_text
            )

            # Fallback to unquoted queries
            if not search_queries:
                search_queries = re.findall(
                    r'(?:\d+\.\s*)?([^(\n]+)\s*\((Google|Google News|Assistant|Agent|Perplexity)\)',
                    queries_text
                )

            if not search_queries:
                logger.info(f"Forecaster {forecaster_id}: No valid search queries found:\n{queries_text}")
                return ""

            logger.info(f"Forecaster {forecaster_id}: Processing {len(search_queries)} search queries")

            # Build tasks for each query
            tasks = []
            query_sources = []

            for match in search_queries:
                if len(match) == 3:
                    _, raw_query, source = match
                else:
                    raw_query, source = match

                query = raw_query.strip().strip('"').strip("'")
                if not query:
                    continue

                # Map Perplexity to Agent for backward compatibility
                if source == "Perplexity":
                    source = "Agent"
                    logger.info(f"Forecaster {forecaster_id}: Mapping Perplexity → Agent for query='{query}'")

                logger.info(f"Forecaster {forecaster_id}: Query='{query}' Source={source}")
                query_sources.append((query, source))

                if source in ("Google", "Google News"):
                    tasks.append(
                        self._google_search_and_scrape(
                            query=query,
                            is_news=(source == "Google News"),
                            question_details=question_details,
                            date_before=question_details.resolution_date,
                        )
                    )
                elif source == "Assistant":
                    tasks.append(self._call_asknews(query))
                elif source == "Agent":
                    tasks.append(self._agentic_search(query))

            if not tasks:
                logger.info(f"Forecaster {forecaster_id}: No tasks generated")
                return ""

            # Execute all tasks
            results = await asyncio.gather(*tasks, return_exceptions=True)

            # Format outputs
            formatted_results = ""
            for (query, source), result in zip(query_sources, results):
                if isinstance(result, Exception):
                    logger.error(f"Forecaster {forecaster_id}: Error for '{query}' → {result}")
                    if source == "Assistant":
                        formatted_results += f"\n<Asknews_articles>\nQuery: {query}\nError retrieving results: {result}\n</Asknews_articles>\n"
                    elif source == "Agent":
                        formatted_results += f"\n<Agent_report>\nQuery: {query}\nError: {result}\n</Agent_report>\n"
                    else:
                        formatted_results += f"\n<Summary query=\"{query}\">\nError: {result}\n</Summary>\n"
                else:
                    logger.info(f"Forecaster {forecaster_id}: Query '{query}' processed successfully")

                    if source == "Assistant":
                        formatted_results += f"\n<Asknews_articles>\nQuery: {query}\n{result}</Asknews_articles>\n"
                    elif source == "Agent":
                        formatted_results += f"\n<Agent_report>\nQuery: {query}\n{result}</Agent_report>\n"
                    else:
                        formatted_results += result

            return formatted_results

        except Exception as e:
            logger.error(f"Forecaster {forecaster_id}: Error processing search queries: {e}")
            traceback.print_exc()
            return "Error processing some search queries. Partial results may be available."

    # -------------------------------------------------------------------------
    # Google Search + Scrape
    # -------------------------------------------------------------------------

    async def _google_search(
        self,
        query: str,
        is_news: bool = False,
        date_before: Optional[str] = None,
    ) -> List[str]:
        """
        Search Google via Serper API.

        Args:
            query: Search query
            is_news: Use Google News instead of web search
            date_before: Only include results before this date

        Returns:
            List of URLs
        """
        if not self.serper_key:
            logger.warning("SERPER_API_KEY not set, skipping Google search")
            return []

        # Clean query
        query = query.replace('"', '').replace("'", '').strip()
        logger.debug(f"[google_search] Query: '{query}' is_news={is_news}")

        search_type = "news" if is_news else "search"
        url = f"https://google.serper.dev/{search_type}"

        try:
            response = await self.http_client.post(
                url,
                headers={
                    'X-API-KEY': self.serper_key,
                    'Content-Type': 'application/json'
                },
                json={"q": query, "num": 20}
            )
            response.raise_for_status()
            data = response.json()

            items = data.get('news' if is_news else 'organic', [])
            logger.debug(f"[google_search] Found {len(items)} raw results")

            # Filter by date if specified
            filtered_items = []
            for item in items:
                if date_before:
                    item_date_str = item.get('date', '')
                    item_date = self._parse_date(item_date_str)
                    if item_date != "Unknown" and self._validate_time(date_before, item_date):
                        filtered_items.append(item)
                    # Skip items we can't validate
                else:
                    filtered_items.append(item)

                if len(filtered_items) >= 12:
                    break

            urls = [item['link'] for item in filtered_items]
            logger.info(f"[google_search] Returning {len(urls)} URLs")
            return urls

        except Exception as e:
            logger.error(f"[google_search] Error: {e}")
            return []

    async def _google_search_and_scrape(
        self,
        query: str,
        is_news: bool,
        question_details: QuestionDetails,
        date_before: Optional[str] = None,
    ) -> str:
        """
        Search Google and scrape/summarize results.

        Args:
            query: Search query
            is_news: Use Google News
            question_details: Question context for summarization
            date_before: Date filter

        Returns:
            Formatted summary string
        """
        logger.debug(f"[google_search_and_scrape] query='{query}' is_news={is_news}")

        try:
            urls = await self._google_search(query, is_news, date_before)

            if not urls:
                logger.warning(f"[google_search_and_scrape] No URLs returned for: '{query}'")
                return f"<Summary query=\"{query}\">No URLs returned from Google.</Summary>\n"

            # Extract content from URLs
            async with FastContentExtractor() as extractor:
                logger.info(f"[google_search_and_scrape] Extracting content from {len(urls)} URLs")
                results = await extractor.extract_content(urls)

            # Summarize top results
            summarize_tasks = []
            valid_urls = []
            max_results = 3

            for url, data in results.items():
                if len(summarize_tasks) >= max_results:
                    break

                content = (data.get('content') or '').strip()
                if len(content.split()) < 100:
                    logger.debug(f"[google_search_and_scrape] Skipping low-content: {url}")
                    continue

                if content:
                    truncated = content[:8000]
                    logger.debug(f"[google_search_and_scrape] Summarizing {len(truncated)} chars from {url}")
                    summarize_tasks.append(
                        self._summarize_article(truncated, question_details)
                    )
                    valid_urls.append(url)

            if not summarize_tasks:
                logger.warning("[google_search_and_scrape] No content to summarize")
                return f"<Summary query=\"{query}\">No usable content extracted from any URL.</Summary>\n"

            summaries = await asyncio.gather(*summarize_tasks, return_exceptions=True)

            # Format output
            output = ""
            for url, summary in zip(valid_urls, summaries):
                if isinstance(summary, Exception):
                    logger.error(f"[google_search_and_scrape] Error summarizing {url}: {summary}")
                    output += f"\n<Summary source=\"{url}\">\nError: {summary}\n</Summary>\n"
                else:
                    output += f"\n<Summary source=\"{url}\">\n{summary}\n</Summary>\n"

            return output

        except Exception as e:
            logger.error(f"[google_search_and_scrape] Error: {e}")
            traceback.print_exc()
            return f"<Summary query=\"{query}\">Error during search and scrape: {e}</Summary>\n"

    async def _summarize_article(
        self,
        article: str,
        question_details: QuestionDetails,
    ) -> str:
        """Summarize an article with question context."""
        prompt = ARTICLE_SUMMARY_PROMPT.format(
            title=question_details.title,
            resolution_criteria=question_details.resolution_criteria,
            fine_print=question_details.fine_print,
            background=question_details.description,
            article=article,
        )

        # Use haiku for summarization (fast, cheap)
        active_models = self.config.get("_active_models", {})
        model = active_models.get(
            "article_summarizer",
            self.config.get("models", {}).get("article_summarizer", "claude-3-haiku-20240307")
        )

        try:
            response = await self.llm.complete(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=1500,
            )
            return response.content
        except Exception as e:
            logger.error(f"Article summarization failed: {e}")
            return f"Error summarizing article: {e}"

    # -------------------------------------------------------------------------
    # AskNews
    # -------------------------------------------------------------------------

    async def _call_asknews(self, query: str) -> str:
        """
        Search AskNews for relevant articles.

        Uses dual strategy: latest news + historical news.
        Uses semaphore to respect AskNews free tier concurrency limits.
        """
        if not self.asknews_client_id or not self.asknews_secret:
            logger.warning("AskNews credentials not set")
            return "AskNews credentials not configured."

        # Use semaphore to limit concurrent AskNews calls (free tier limit)
        async with self._asknews_semaphore:
            try:
                from asknews_sdk import AskNewsSDK

                ask = AskNewsSDK(
                    client_id=self.asknews_client_id,
                    client_secret=self.asknews_secret,
                    scopes=set(["news"])
                )

                # Run searches sequentially with rate limit delay
                # AskNews Pro tier (via Metaculus): 1 request per 10 seconds, add 2s buffer
                rate_limit_delay = 12

                logger.debug(f"[call_asknews] Searching latest news for: {query[:50]}...")
                hot_response = await asyncio.to_thread(
                    ask.news.search_news,
                    query=query,
                    n_articles=8,
                    return_type="both",
                    strategy="latest news"
                )

                # Rate limit delay between calls
                logger.debug(f"[call_asknews] Waiting {rate_limit_delay}s for rate limit...")
                await asyncio.sleep(rate_limit_delay)

                logger.debug(f"[call_asknews] Searching historical news for: {query[:50]}...")
                historical_response = await asyncio.to_thread(
                    ask.news.search_news,
                    query=query,
                    n_articles=8,
                    return_type="both",
                    strategy="news knowledge"
                )

                # Format results
                formatted_articles = "Here are the relevant news articles:\n\n"

                hot_articles = hot_response.as_dicts
                if hot_articles:
                    hot_articles = [article.__dict__ for article in hot_articles]
                    hot_articles = sorted(hot_articles, key=lambda x: x["pub_date"], reverse=True)

                    for article in hot_articles:
                        pub_date = article["pub_date"].strftime("%B %d, %Y %I:%M %p")
                        formatted_articles += (
                            f"**{article['eng_title']}**\n"
                            f"{article['summary']}\n"
                            f"Original language: {article['language']}\n"
                            f"Publish date: {pub_date}\n"
                            f"Source:[{article['source_id']}]({article['article_url']})\n\n"
                        )

                historical_articles = historical_response.as_dicts
                if historical_articles:
                    historical_articles = [article.__dict__ for article in historical_articles]
                    historical_articles = sorted(historical_articles, key=lambda x: x["pub_date"], reverse=True)

                    for article in historical_articles:
                        pub_date = article["pub_date"].strftime("%B %d, %Y %I:%M %p")
                        formatted_articles += (
                            f"**{article['eng_title']}**\n"
                            f"{article['summary']}\n"
                            f"Original language: {article['language']}\n"
                            f"Publish date: {pub_date}\n"
                            f"Source:[{article['source_id']}]({article['article_url']})\n\n"
                        )

                if not hot_articles and not historical_articles:
                    formatted_articles += "No articles were found.\n\n"

                return formatted_articles

            except Exception as e:
                logger.error(f"[call_asknews] Error: {e}")
                return f"Error retrieving news articles: {e}"

    # -------------------------------------------------------------------------
    # Agentic Search
    # -------------------------------------------------------------------------

    async def _agentic_search(self, query: str) -> str:
        """
        Perform iterative agentic search using LLM.

        The LLM generates search queries, analyzes results, and iterates
        until it has enough information or reaches max steps.
        """
        logger.info(f"[agentic_search] Starting research for: {query}")

        max_steps = self.config.get("research", {}).get("agentic_search_max_steps", 7)
        current_analysis = ""
        all_search_queries: List[str] = []
        search_results = ""

        # Get model for agentic search from active models (respects mode)
        active_models = self.config.get("_active_models", {})
        model = active_models.get(
            "agentic_search",
            # Fallback: check if there's a query_generator, else default to haiku
            active_models.get("query_generator", "openrouter/anthropic/claude-3.5-haiku")
        )

        for step in range(max_steps):
            try:
                # Build prompt
                if step == 0:
                    prompt = INITIAL_SEARCH_PROMPT.format(query=query)
                else:
                    if current_analysis:
                        previous_section = (
                            f"Your previous analysis:\n{current_analysis}\n\n"
                            f"Previous search queries used: {', '.join(all_search_queries)}\n"
                        )
                    else:
                        previous_section = f"Previous search queries used: {', '.join(all_search_queries)}\n"

                    prompt = CONTINUATION_SEARCH_PROMPT.format(
                        query=query,
                        previous_section=previous_section,
                        search_results=search_results,
                    )

                logger.info(f"[agentic_search] Step {step + 1}: Calling LLM")
                response = await self.llm.complete(
                    model=model,
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=4000,
                )

                response_text = response.content

                # Parse analysis
                analysis_match = re.search(
                    r'Analysis:\s*(.*?)(?=Search queries:|$)',
                    response_text,
                    re.DOTALL
                )
                if not analysis_match:
                    logger.warning(f"[agentic_search] Could not parse analysis from response")
                    return f"Error: Failed to parse analysis at step {step + 1}"

                if step > 0:
                    current_analysis = analysis_match.group(1).strip()
                    logger.info(f"[agentic_search] Step {step + 1}: Analysis updated ({len(current_analysis)} chars)")

                # Check for search queries
                search_queries_match = re.search(
                    r'Search queries:\s*(.*)',
                    response_text,
                    re.DOTALL
                )

                if step == 0 and not search_queries_match:
                    logger.warning(f"[agentic_search] No search queries in initial response")
                    return "Error: Failed to generate initial search queries"

                if not search_queries_match or step == max_steps - 1:
                    if step > 0:
                        logger.info(f"[agentic_search] Research complete at step {step + 1}")
                        break

                # Extract queries with sources
                queries_text = search_queries_match.group(1).strip()
                search_queries_with_source = re.findall(
                    r'\d+\.\s*([^(]+?)\s*\((Google|Google News)\)',
                    queries_text
                )

                if not search_queries_with_source:
                    if step == 0:
                        logger.warning(f"[agentic_search] No valid queries in initial response")
                        return "Error: Failed to parse initial search queries"
                    else:
                        logger.info(f"[agentic_search] No new queries, completing research")
                        break

                # Limit to 5 queries
                search_queries_with_source = [
                    (q.strip(), source)
                    for q, source in search_queries_with_source[:5]
                ]

                logger.info(f"[agentic_search] Step {step + 1}: Found {len(search_queries_with_source)} queries")
                all_search_queries.extend([q for q, _ in search_queries_with_source])

                # Execute searches
                search_tasks = []
                for sq, source in search_queries_with_source:
                    logger.debug(f"[agentic_search] Searching: {sq} (Source: {source})")
                    search_tasks.append(
                        self._google_search_agentic(sq, is_news=(source == "Google News"))
                    )

                search_results_list = await asyncio.gather(*search_tasks, return_exceptions=True)

                # Format search results
                search_results = ""
                for (sq, source), result in zip(search_queries_with_source, search_results_list):
                    if isinstance(result, Exception):
                        search_results += f"\nSearch query: {sq} (Source: {source})\nError: {result}\n"
                    else:
                        search_results += f"\nSearch query: {sq} (Source: {source})\n{result}\n"

                logger.info(f"[agentic_search] Step {step + 1}: Search complete, {len(search_results)} chars")

            except Exception as e:
                logger.error(f"[agentic_search] Error at step {step + 1}: {e}")
                if current_analysis:
                    break
                else:
                    return f"Error during agentic search: {e}"

        if not current_analysis:
            return "Error: No analysis was generated during the research process"

        logger.info(f"[agentic_search] Complete: {step + 1} steps")
        return current_analysis

    async def _google_search_agentic(self, query: str, is_news: bool = False) -> str:
        """
        Google search that returns raw content (no summarization).

        Used by agentic search where the agent analyzes raw content.
        """
        logger.debug(f"[google_search_agentic] query='{query}' is_news={is_news}")

        try:
            urls = await self._google_search(query, is_news)

            if not urls:
                return f"<RawContent query=\"{query}\">No URLs returned from Google.</RawContent>\n"

            async with FastContentExtractor() as extractor:
                results = await extractor.extract_content(urls)

            output = ""
            max_results = 3
            results_count = 0

            for url, data in results.items():
                if results_count >= max_results:
                    break

                content = (data.get('content') or '').strip()
                if len(content.split()) < 100:
                    continue

                if content:
                    truncated = content[:8000]
                    output += f"\n<RawContent source=\"{url}\">\n{truncated}\n</RawContent>\n"
                    results_count += 1

            if not output:
                return f"<RawContent query=\"{query}\">No usable content extracted.</RawContent>\n"

            return output

        except Exception as e:
            logger.error(f"[google_search_agentic] Error: {e}")
            return f"<RawContent query=\"{query}\">Error during search: {e}</RawContent>\n"

    # -------------------------------------------------------------------------
    # Utility methods
    # -------------------------------------------------------------------------

    def _parse_date(self, date_str: str) -> str:
        """Parse date string to standardized format."""
        if not date_str:
            return "Unknown"
        parsed_date = dateparser.parse(date_str, settings={'STRICT_PARSING': False})
        if parsed_date:
            return parsed_date.strftime("%b %d, %Y")
        return "Unknown"

    def _validate_time(self, before_date_str: str, source_date_str: str) -> bool:
        """Check if source date is before the cutoff date."""
        if source_date_str == "Unknown":
            return False
        before_date = dateparser.parse(before_date_str)
        source_date = dateparser.parse(source_date_str)
        if before_date and source_date:
            return source_date <= before_date
        return False


# -------------------------------------------------------------------------
# Convenience functions
# -------------------------------------------------------------------------

async def process_forecaster_queries(
    response: str,
    forecaster_id: str,
    question_details: dict,
    config: dict,
    llm_client: Optional[LLMClient] = None,
) -> str:
    """
    Convenience function to process search queries from a forecaster response.

    Args:
        response: Forecaster's response containing search queries
        forecaster_id: ID for logging
        question_details: Dict with title, resolution_criteria, fine_print, description
        config: Configuration dict
        llm_client: Optional LLMClient instance

    Returns:
        Formatted search results string
    """
    qd = QuestionDetails(
        title=question_details.get("title", ""),
        resolution_criteria=question_details.get("resolution_criteria", ""),
        fine_print=question_details.get("fine_print", ""),
        description=question_details.get("description", ""),
        resolution_date=question_details.get("resolution_date"),
    )

    async with SearchPipeline(config, llm_client) as pipeline:
        return await pipeline.process_search_queries(response, forecaster_id, qd)
